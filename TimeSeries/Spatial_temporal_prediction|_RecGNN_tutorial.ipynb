{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CristinaMarsh/Reproduce/blob/main/TimeSeries/Spatial_temporal_prediction%7C_RecGNN_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2lmOu530iov"
      },
      "source": [
        "# Tutorial 9: Recurrent GNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHkxN-n90ioy"
      },
      "source": [
        "In this tutorial we will implement an approximation of the Graph Neural Network Model (without enforcing contraction map) and analyze the GatedGraph Convolution of Pytorch Geometric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GZdwEst0ioz",
        "outputId": "c6297db3-355d-49a5-cf69-0e2f0c8e92c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.11.0+cu113\n",
            "\u001b[K     |████████████████████████████████| 7.9 MB 4.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 4.2 MB/s \n",
            "\u001b[?25h  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n56P9ZXV0io0"
      },
      "outputs": [],
      "source": [
        "import os.path as osp\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch_geometric.transforms as T\n",
        "import torch_geometric\n",
        "from torch_geometric.datasets import Planetoid, TUDataset\n",
        "from torch_geometric.data import DataLoader\n",
        "from torch_geometric.nn.inits import uniform\n",
        "from torch.nn import Parameter as Param\n",
        "from torch import Tensor \n",
        "torch.manual_seed(42)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device = \"cpu\"\n",
        "from torch_geometric.nn.conv import MessagePassing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_uMMt3n0io1",
        "outputId": "00036034-b4f6-4cdc-e533-6863cbd7c745",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
            "Processing...\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "dataset = 'Cora'\n",
        "transform = T.Compose([T.TargetIndegree(),\n",
        "])\n",
        "path = osp.join('data', dataset)\n",
        "dataset = Planetoid(path, dataset, transform=transform)\n",
        "data = dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73Q5r0Uz0io2"
      },
      "outputs": [],
      "source": [
        "dataset = 'Cora'\n",
        "path = osp.join('data', dataset)\n",
        "dataset = Planetoid(path, dataset, transform=T.NormalizeFeatures())\n",
        "data = dataset[0]\n",
        "data = data.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lcHqR760io3"
      },
      "source": [
        "### Graph Neural Network Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KL4p8sxO0io3"
      },
      "source": [
        "<img src=\"https://github.com/AntonioLonga/PytorchGeometricTutorial/blob/main/Tutorial9/transition.png?raw=1\" width=\"500\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqJf5Bgv0io3"
      },
      "source": [
        "<img src=\"https://github.com/AntonioLonga/PytorchGeometricTutorial/blob/main/Tutorial9/output.png?raw=1\" width=\"500\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sb3N9DU0io4"
      },
      "source": [
        "The MLP class is used to instantiate the transition and output functions as simple feed forard networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSb3wYpp0io4"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim, hid_dims, out_dim):\n",
        "        super(MLP, self).__init__()\n",
        "\n",
        "        self.mlp = nn.Sequential()\n",
        "        dims = [input_dim] + hid_dims + [out_dim]\n",
        "        for i in range(len(dims)-1):\n",
        "            self.mlp.add_module('lay_{}'.format(i),nn.Linear(in_features=dims[i], out_features=dims[i+1]))\n",
        "            if i+2 < len(dims):\n",
        "                self.mlp.add_module('act_{}'.format(i), nn.Tanh())\n",
        "    def reset_parameters(self):\n",
        "        for i, l in enumerate(self.mlp):\n",
        "            if type(l) == nn.Linear:\n",
        "                nn.init.xavier_normal_(l.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.mlp(x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yf9_7DDU0io5"
      },
      "source": [
        "The GNNM calss puts together the state propagations and the readout of the nodes' states."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NE_RjOMU0io5"
      },
      "outputs": [],
      "source": [
        "class GNNM(MessagePassing):\n",
        "    def __init__(self, n_nodes, out_channels, features_dim, hid_dims, num_layers = 50, eps=1e-3, aggr = 'add',\n",
        "                 bias = True, **kwargs):\n",
        "        super(GNNM, self).__init__(aggr=aggr, **kwargs)\n",
        "\n",
        "        self.node_states = Param(torch.zeros((n_nodes, features_dim)), requires_grad=False)\n",
        "        self.out_channels = out_channels\n",
        "        self.eps = eps\n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "        self.transition = MLP(features_dim, hid_dims, features_dim)\n",
        "        self.readout = MLP(features_dim, hid_dims, out_channels)\n",
        "        \n",
        "        self.reset_parameters()\n",
        "        print(self.transition)\n",
        "        print(self.readout)\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.transition.reset_parameters()\n",
        "        self.readout.reset_parameters()\n",
        "        \n",
        "    def forward(self): \n",
        "        edge_index = data.edge_index\n",
        "        edge_weight = data.edge_attr\n",
        "        node_states = self.node_states\n",
        "        for i in range(self.num_layers):\n",
        "            m = self.propagate(edge_index, x=node_states, edge_weight=edge_weight,\n",
        "                               size=None)\n",
        "            new_states = self.transition(m)\n",
        "            with torch.no_grad():\n",
        "                distance = torch.norm(new_states - node_states, dim=1)\n",
        "                convergence = distance < self.eps\n",
        "            node_states = new_states\n",
        "            if convergence.all():\n",
        "                break\n",
        "            \n",
        "        out = self.readout(node_states)\n",
        "        \n",
        "        return F.log_softmax(out, dim=-1)\n",
        "\n",
        "    def message(self, x_j, edge_weight):\n",
        "        return x_j if edge_weight is None else edge_weight.view(-1, 1) * x_j\n",
        "\n",
        "    def message_and_aggregate(self, adj_t, x) :\n",
        "        return matmul(adj_t, x, reduce=self.aggr)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return '{}({}, num_layers={})'.format(self.__class__.__name__,\n",
        "                                              self.out_channels,\n",
        "                                              self.num_layers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "b-gj2-bd0io6",
        "outputId": "02464a3d-672d-4e9d-a8b0-865288635678",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP(\n",
            "  (mlp): Sequential(\n",
            "    (lay_0): Linear(in_features=32, out_features=64, bias=True)\n",
            "    (act_0): Tanh()\n",
            "    (lay_1): Linear(in_features=64, out_features=64, bias=True)\n",
            "    (act_1): Tanh()\n",
            "    (lay_2): Linear(in_features=64, out_features=64, bias=True)\n",
            "    (act_2): Tanh()\n",
            "    (lay_3): Linear(in_features=64, out_features=64, bias=True)\n",
            "    (act_3): Tanh()\n",
            "    (lay_4): Linear(in_features=64, out_features=64, bias=True)\n",
            "    (act_4): Tanh()\n",
            "    (lay_5): Linear(in_features=64, out_features=32, bias=True)\n",
            "  )\n",
            ")\n",
            "MLP(\n",
            "  (mlp): Sequential(\n",
            "    (lay_0): Linear(in_features=32, out_features=64, bias=True)\n",
            "    (act_0): Tanh()\n",
            "    (lay_1): Linear(in_features=64, out_features=64, bias=True)\n",
            "    (act_1): Tanh()\n",
            "    (lay_2): Linear(in_features=64, out_features=64, bias=True)\n",
            "    (act_2): Tanh()\n",
            "    (lay_3): Linear(in_features=64, out_features=64, bias=True)\n",
            "    (act_3): Tanh()\n",
            "    (lay_4): Linear(in_features=64, out_features=64, bias=True)\n",
            "    (act_4): Tanh()\n",
            "    (lay_5): Linear(in_features=64, out_features=7, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch_geometric/deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Train Acc: 0.18571, Val Acc: 0.21000, Test Acc: 0.18800\n",
            "Epoch: 002, Train Acc: 0.15000, Val Acc: 0.16600, Test Acc: 0.14400\n",
            "Epoch: 003, Train Acc: 0.19286, Val Acc: 0.13400, Test Acc: 0.11300\n",
            "Epoch: 004, Train Acc: 0.15714, Val Acc: 0.12800, Test Acc: 0.09400\n",
            "Epoch: 005, Train Acc: 0.10714, Val Acc: 0.12600, Test Acc: 0.12600\n",
            "Epoch: 006, Train Acc: 0.17857, Val Acc: 0.21200, Test Acc: 0.20400\n",
            "Epoch: 007, Train Acc: 0.17857, Val Acc: 0.20600, Test Acc: 0.20000\n",
            "Epoch: 008, Train Acc: 0.17857, Val Acc: 0.19000, Test Acc: 0.19700\n",
            "Epoch: 009, Train Acc: 0.22857, Val Acc: 0.17400, Test Acc: 0.17100\n",
            "Epoch: 010, Train Acc: 0.21429, Val Acc: 0.12800, Test Acc: 0.11000\n",
            "Epoch: 011, Train Acc: 0.20714, Val Acc: 0.14000, Test Acc: 0.11500\n",
            "Epoch: 012, Train Acc: 0.20714, Val Acc: 0.15400, Test Acc: 0.12700\n",
            "Epoch: 013, Train Acc: 0.20714, Val Acc: 0.16800, Test Acc: 0.13000\n",
            "Epoch: 014, Train Acc: 0.21429, Val Acc: 0.15800, Test Acc: 0.12600\n",
            "Epoch: 015, Train Acc: 0.20000, Val Acc: 0.17200, Test Acc: 0.14000\n",
            "Epoch: 016, Train Acc: 0.24286, Val Acc: 0.18000, Test Acc: 0.15600\n",
            "Epoch: 017, Train Acc: 0.26429, Val Acc: 0.17400, Test Acc: 0.16000\n",
            "Epoch: 018, Train Acc: 0.25714, Val Acc: 0.19200, Test Acc: 0.15900\n",
            "Epoch: 019, Train Acc: 0.25714, Val Acc: 0.18000, Test Acc: 0.15800\n",
            "Epoch: 020, Train Acc: 0.24286, Val Acc: 0.17800, Test Acc: 0.14700\n",
            "Epoch: 021, Train Acc: 0.24286, Val Acc: 0.15200, Test Acc: 0.13600\n",
            "Epoch: 022, Train Acc: 0.23571, Val Acc: 0.16600, Test Acc: 0.12500\n",
            "Epoch: 023, Train Acc: 0.25000, Val Acc: 0.17400, Test Acc: 0.12900\n",
            "Epoch: 024, Train Acc: 0.25000, Val Acc: 0.17800, Test Acc: 0.13700\n",
            "Epoch: 025, Train Acc: 0.25000, Val Acc: 0.18200, Test Acc: 0.14000\n",
            "Epoch: 026, Train Acc: 0.25000, Val Acc: 0.19200, Test Acc: 0.15200\n",
            "Epoch: 027, Train Acc: 0.27143, Val Acc: 0.18400, Test Acc: 0.15800\n",
            "Epoch: 028, Train Acc: 0.25000, Val Acc: 0.18600, Test Acc: 0.14700\n",
            "Epoch: 029, Train Acc: 0.25714, Val Acc: 0.18400, Test Acc: 0.14700\n",
            "Epoch: 030, Train Acc: 0.25000, Val Acc: 0.17600, Test Acc: 0.14700\n",
            "Epoch: 031, Train Acc: 0.23571, Val Acc: 0.17000, Test Acc: 0.14100\n",
            "Epoch: 032, Train Acc: 0.23571, Val Acc: 0.17000, Test Acc: 0.14100\n",
            "Epoch: 033, Train Acc: 0.25714, Val Acc: 0.18200, Test Acc: 0.14800\n",
            "Epoch: 034, Train Acc: 0.26429, Val Acc: 0.19000, Test Acc: 0.15400\n",
            "Epoch: 035, Train Acc: 0.27143, Val Acc: 0.19000, Test Acc: 0.15500\n",
            "Epoch: 036, Train Acc: 0.25000, Val Acc: 0.19600, Test Acc: 0.15000\n",
            "Epoch: 037, Train Acc: 0.27143, Val Acc: 0.17800, Test Acc: 0.15400\n",
            "Epoch: 038, Train Acc: 0.22857, Val Acc: 0.17400, Test Acc: 0.13200\n",
            "Epoch: 039, Train Acc: 0.24286, Val Acc: 0.16800, Test Acc: 0.14900\n",
            "Epoch: 040, Train Acc: 0.25714, Val Acc: 0.18800, Test Acc: 0.15200\n",
            "Epoch: 041, Train Acc: 0.22143, Val Acc: 0.17200, Test Acc: 0.13900\n",
            "Epoch: 042, Train Acc: 0.25714, Val Acc: 0.17400, Test Acc: 0.15300\n",
            "Epoch: 043, Train Acc: 0.22857, Val Acc: 0.18200, Test Acc: 0.15500\n",
            "Epoch: 044, Train Acc: 0.23571, Val Acc: 0.17600, Test Acc: 0.14300\n",
            "Epoch: 045, Train Acc: 0.27857, Val Acc: 0.18600, Test Acc: 0.15900\n",
            "Epoch: 046, Train Acc: 0.23571, Val Acc: 0.17600, Test Acc: 0.15300\n",
            "Epoch: 047, Train Acc: 0.25000, Val Acc: 0.18200, Test Acc: 0.15200\n",
            "Epoch: 048, Train Acc: 0.26429, Val Acc: 0.18200, Test Acc: 0.15900\n",
            "Epoch: 049, Train Acc: 0.23571, Val Acc: 0.18600, Test Acc: 0.14400\n",
            "Epoch: 050, Train Acc: 0.28571, Val Acc: 0.17000, Test Acc: 0.15000\n"
          ]
        }
      ],
      "source": [
        "model = GNNM(data.num_nodes, dataset.num_classes, 32, [64,64,64,64,64], eps=0.01).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "test_dataset = dataset[:len(dataset) // 10]\n",
        "train_dataset = dataset[len(dataset) // 10:]\n",
        "test_loader = DataLoader(test_dataset)\n",
        "train_loader = DataLoader(train_dataset)\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    loss_fn(model()[data.train_mask], data.y[data.train_mask]).backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "def test():\n",
        "    model.eval()\n",
        "    logits, accs = model(), []\n",
        "    for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
        "        pred = logits[mask].max(1)[1]\n",
        "        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
        "        accs.append(acc)\n",
        "    return accs\n",
        "\n",
        "\n",
        "for epoch in range(1, 51):\n",
        "    train()\n",
        "    accs = test()\n",
        "    train_acc = accs[0]\n",
        "    val_acc = accs[1]\n",
        "    test_acc = accs[2]\n",
        "    print('Epoch: {:03d}, Train Acc: {:.5f}, '\n",
        "          'Val Acc: {:.5f}, Test Acc: {:.5f}'.format(epoch, train_acc,\n",
        "                                                       val_acc, test_acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrTntmbp0io6"
      },
      "source": [
        "### Gated Graph Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eV6blnlf0io6"
      },
      "outputs": [],
      "source": [
        "class GatedGraphConv(MessagePassing):\n",
        "    \n",
        "    def __init__(self, out_channels, num_layers, aggr = 'add',\n",
        "                 bias = True, **kwargs):\n",
        "        super(GatedGraphConv, self).__init__(aggr=aggr, **kwargs)\n",
        "\n",
        "        self.out_channels = out_channels\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.weight = Param(Tensor(num_layers, out_channels, out_channels))\n",
        "        self.rnn = torch.nn.GRUCell(out_channels, out_channels, bias=bias)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        uniform(self.out_channels, self.weight)\n",
        "        self.rnn.reset_parameters()\n",
        "\n",
        "    def forward(self, data):\n",
        "        \"\"\"\"\"\"\n",
        "        x = data.x\n",
        "        edge_index = data.edge_index\n",
        "        edge_weight = data.edge_attr\n",
        "        if x.size(-1) > self.out_channels:\n",
        "            raise ValueError('The number of input channels is not allowed to '\n",
        "                             'be larger than the number of output channels')\n",
        "\n",
        "        if x.size(-1) < self.out_channels:\n",
        "            zero = x.new_zeros(x.size(0), self.out_channels - x.size(-1))\n",
        "            x = torch.cat([x, zero], dim=1)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            m = torch.matmul(x, self.weight[i])\n",
        "            m = self.propagate(edge_index, x=m, edge_weight=edge_weight,\n",
        "                               size=None)\n",
        "            x = self.rnn(m, x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def message(self, x_j, edge_weight):\n",
        "        return x_j if edge_weight is None else edge_weight.view(-1, 1) * x_j\n",
        "\n",
        "    def message_and_aggregate(self, adj_t, x):\n",
        "        return matmul(adj_t, x, reduce=self.aggr)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return '{}({}, num_layers={})'.format(self.__class__.__name__,\n",
        "                                              self.out_channels,\n",
        "                                              self.num_layers)\n",
        "\n",
        "class GGNN(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GGNN, self).__init__()\n",
        "        \n",
        "        self.conv = GatedGraphConv(1433, 3)\n",
        "        self.mlp = MLP(1433, [32,32,32], dataset.num_classes)\n",
        "        \n",
        "    def forward(self):\n",
        "        x = self.conv(data)\n",
        "        x = self.mlp(x)\n",
        "        return F.log_softmax(x, dim=-1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "biLO6R500io7",
        "outputId": "d379ebca-1618-4867-a1f5-864bc8016407",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch_geometric/deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Train Acc: 0.15000, Val Acc: 0.13600, Test Acc: 0.13100\n",
            "Epoch: 002, Train Acc: 0.17857, Val Acc: 0.25400, Test Acc: 0.23500\n",
            "Epoch: 003, Train Acc: 0.18571, Val Acc: 0.13400, Test Acc: 0.13800\n",
            "Epoch: 004, Train Acc: 0.29286, Val Acc: 0.22000, Test Acc: 0.22500\n",
            "Epoch: 005, Train Acc: 0.36429, Val Acc: 0.34800, Test Acc: 0.33200\n",
            "Epoch: 006, Train Acc: 0.40000, Val Acc: 0.36800, Test Acc: 0.34100\n",
            "Epoch: 007, Train Acc: 0.40714, Val Acc: 0.37800, Test Acc: 0.35300\n",
            "Epoch: 008, Train Acc: 0.42857, Val Acc: 0.41800, Test Acc: 0.39000\n",
            "Epoch: 009, Train Acc: 0.54286, Val Acc: 0.48800, Test Acc: 0.46900\n",
            "Epoch: 010, Train Acc: 0.55714, Val Acc: 0.51800, Test Acc: 0.50300\n",
            "Epoch: 011, Train Acc: 0.50714, Val Acc: 0.48400, Test Acc: 0.47100\n",
            "Epoch: 012, Train Acc: 0.46429, Val Acc: 0.46400, Test Acc: 0.46000\n",
            "Epoch: 013, Train Acc: 0.50000, Val Acc: 0.48600, Test Acc: 0.49000\n",
            "Epoch: 014, Train Acc: 0.57857, Val Acc: 0.51600, Test Acc: 0.51200\n",
            "Epoch: 015, Train Acc: 0.60714, Val Acc: 0.54000, Test Acc: 0.53400\n",
            "Epoch: 016, Train Acc: 0.68571, Val Acc: 0.53600, Test Acc: 0.54000\n",
            "Epoch: 017, Train Acc: 0.69286, Val Acc: 0.54600, Test Acc: 0.55200\n",
            "Epoch: 018, Train Acc: 0.66429, Val Acc: 0.55000, Test Acc: 0.55000\n",
            "Epoch: 019, Train Acc: 0.70000, Val Acc: 0.54000, Test Acc: 0.55100\n",
            "Epoch: 020, Train Acc: 0.75714, Val Acc: 0.53000, Test Acc: 0.54800\n",
            "Epoch: 021, Train Acc: 0.77857, Val Acc: 0.57000, Test Acc: 0.58700\n",
            "Epoch: 022, Train Acc: 0.78571, Val Acc: 0.57200, Test Acc: 0.60900\n",
            "Epoch: 023, Train Acc: 0.80000, Val Acc: 0.57400, Test Acc: 0.61600\n",
            "Epoch: 024, Train Acc: 0.80000, Val Acc: 0.56400, Test Acc: 0.61300\n",
            "Epoch: 025, Train Acc: 0.81429, Val Acc: 0.57000, Test Acc: 0.61200\n",
            "Epoch: 026, Train Acc: 0.80714, Val Acc: 0.54800, Test Acc: 0.57600\n",
            "Epoch: 027, Train Acc: 0.81429, Val Acc: 0.56200, Test Acc: 0.57400\n",
            "Epoch: 028, Train Acc: 0.82143, Val Acc: 0.56200, Test Acc: 0.58200\n",
            "Epoch: 029, Train Acc: 0.82857, Val Acc: 0.55800, Test Acc: 0.58700\n",
            "Epoch: 030, Train Acc: 0.82857, Val Acc: 0.55600, Test Acc: 0.58900\n",
            "Epoch: 031, Train Acc: 0.82857, Val Acc: 0.55400, Test Acc: 0.58400\n",
            "Epoch: 032, Train Acc: 0.91429, Val Acc: 0.58400, Test Acc: 0.61700\n",
            "Epoch: 033, Train Acc: 0.92857, Val Acc: 0.59400, Test Acc: 0.64300\n",
            "Epoch: 034, Train Acc: 0.94286, Val Acc: 0.59800, Test Acc: 0.64500\n",
            "Epoch: 035, Train Acc: 0.94286, Val Acc: 0.60200, Test Acc: 0.64600\n",
            "Epoch: 036, Train Acc: 0.95714, Val Acc: 0.62400, Test Acc: 0.66000\n",
            "Epoch: 037, Train Acc: 0.95714, Val Acc: 0.62800, Test Acc: 0.66300\n",
            "Epoch: 038, Train Acc: 0.96429, Val Acc: 0.64000, Test Acc: 0.67100\n",
            "Epoch: 039, Train Acc: 0.96429, Val Acc: 0.64600, Test Acc: 0.67600\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-fde0cb8923bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m51\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0maccs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-fde0cb8923bf>\u001b[0m in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_mask'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'val_mask'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test_mask'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-3b99ec8bab8e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-3b99ec8bab8e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     34\u001b[0m             m = self.propagate(edge_index, x=m, edge_weight=edge_weight,\n\u001b[1;32m     35\u001b[0m                                size=None)\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1268\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_ih\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_hh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1270\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_ih\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_hh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1271\u001b[0m         )\n\u001b[1;32m   1272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "model = GGNN().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "test_dataset = dataset[:len(dataset) // 10]\n",
        "train_dataset = dataset[len(dataset) // 10:]\n",
        "test_loader = DataLoader(test_dataset)\n",
        "train_loader = DataLoader(train_dataset)\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    loss_fn(model()[data.train_mask], data.y[data.train_mask]).backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "def test():\n",
        "    model.eval()\n",
        "    logits, accs = model(), []\n",
        "    for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
        "        pred = logits[mask].max(1)[1]\n",
        "        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
        "        accs.append(acc)\n",
        "    return accs\n",
        "\n",
        "\n",
        "for epoch in range(1, 51):\n",
        "    train()\n",
        "    accs = test()\n",
        "    train_acc = accs[0]\n",
        "    val_acc = accs[1]\n",
        "    test_acc = accs[2]\n",
        "    print('Epoch: {:03d}, Train Acc: {:.5f}, '\n",
        "          'Val Acc: {:.5f}, Test Acc: {:.5f}'.format(epoch, train_acc,\n",
        "                                                       val_acc, test_acc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5sRLtNVK0io7"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQRVMECV0io7"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "name": "Spatial temporal prediction| RecGNN_tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}