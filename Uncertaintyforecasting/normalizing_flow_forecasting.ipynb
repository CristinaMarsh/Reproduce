{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMoyDLJ44gTTNxgh7TGZE8K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CristinaMarsh/Reproduce/blob/main/Uncertaintyforecasting/normalizing_flow_forecasting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjWPGoGLg_-a",
        "outputId": "cbf839e2-b866-4ec9-c7a8-5452cf2dbec2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Conditional-normalizing-flow-for-wind-power-forecasting'...\n",
            "remote: Enumerating objects: 22, done.\u001b[K\n",
            "remote: Counting objects: 100% (22/22), done.\u001b[K\n",
            "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
            "remote: Total 22 (delta 5), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (22/22), 752.32 KiB | 4.20 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "! git clone https://github.com/honglinwen/Conditional-normalizing-flow-for-wind-power-forecasting.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install UMNN\n",
        "!pip install properscoring\n",
        "!pip install git+https://github.com/bayesiains/nflows.git\n",
        "!pip install torch-summary\n",
        "     "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcIb-wKuhI2t",
        "outputId": "8101752d-68ec-471b-ee36-8b975436683f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting UMNN\n",
            "  Downloading UMNN-1.68.tar.gz (28 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: UMNN\n",
            "  Building wheel for UMNN (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for UMNN: filename=UMNN-1.68-py3-none-any.whl size=36282 sha256=b22d86c0ce28ec2deaf462d04568ec0d2b3cd200dc9aa35eea8929028c212fc0\n",
            "  Stored in directory: /root/.cache/pip/wheels/1d/ee/7e/95a477fd53650135d7dfac61e01b4f0e6f18e2a507dd5202da\n",
            "Successfully built UMNN\n",
            "Installing collected packages: UMNN\n",
            "Successfully installed UMNN-1.68\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting properscoring\n",
            "  Downloading properscoring-0.1-py2.py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from properscoring) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from properscoring) (1.10.1)\n",
            "Installing collected packages: properscoring\n",
            "Successfully installed properscoring-0.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/bayesiains/nflows.git\n",
            "  Cloning https://github.com/bayesiains/nflows.git to /tmp/pip-req-build-h_wru7fi\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/bayesiains/nflows.git /tmp/pip-req-build-h_wru7fi\n",
            "  Encountered 2 file(s) that should have been pointers, but weren't:\n",
            "        examples/conditional_moons.ipynb\n",
            "        examples/moons.ipynb\n",
            "  Resolved https://github.com/bayesiains/nflows.git to commit 569c8ad50941824ccb07aa3a3eb59c85721e0c5f\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from nflows==0.14) (3.5.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from nflows==0.14) (1.22.4)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.9/dist-packages (from nflows==0.14) (2.11.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from nflows==0.14) (1.13.1+cu116)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from nflows==0.14) (4.65.0)\n",
            "Requirement already satisfied: umnn in /usr/local/lib/python3.9/dist-packages (from nflows==0.14) (1.68)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->nflows==0.14) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib->nflows==0.14) (2.8.2)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->nflows==0.14) (4.39.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->nflows==0.14) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->nflows==0.14) (3.0.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->nflows==0.14) (23.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->nflows==0.14) (0.11.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.9/dist-packages (from tensorboard->nflows==0.14) (0.38.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard->nflows==0.14) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard->nflows==0.14) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard->nflows==0.14) (2.16.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.9/dist-packages (from tensorboard->nflows==0.14) (1.4.0)\n",
            "Requirement already satisfied: protobuf<4,>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorboard->nflows==0.14) (3.19.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard->nflows==0.14) (0.6.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard->nflows==0.14) (2.25.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard->nflows==0.14) (2.2.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard->nflows==0.14) (3.4.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard->nflows==0.14) (1.51.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard->nflows==0.14) (57.4.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch->nflows==0.14) (4.5.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard->nflows==0.14) (1.15.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard->nflows==0.14) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard->nflows==0.14) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard->nflows==0.14) (5.3.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->nflows==0.14) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard->nflows==0.14) (6.0.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard->nflows==0.14) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard->nflows==0.14) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard->nflows==0.14) (1.26.14)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard->nflows==0.14) (2.10)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard->nflows==0.14) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->nflows==0.14) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->nflows==0.14) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->nflows==0.14) (3.2.2)\n",
            "Building wheels for collected packages: nflows\n",
            "  Building wheel for nflows (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nflows: filename=nflows-0.14-py3-none-any.whl size=79807 sha256=5f15d1fe42d7ffede4152f04eb4ab69af6ec1a2e5e6ae434f06e1e44d7028b00\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-u8u5v6uk/wheels/70/30/d6/f52415a3ab3a2f4734f50b2323ddce82a75d59c346eddad137\n",
            "Successfully built nflows\n",
            "Installing collected packages: nflows\n",
            "Successfully installed nflows-0.14\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch-summary\n",
            "  Downloading torch_summary-1.4.5-py3-none-any.whl (16 kB)\n",
            "Installing collected packages: torch-summary\n",
            "Successfully installed torch-summary-1.4.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "\n",
        "from nflows.flows.base import Flow\n",
        "from nflows.distributions.normal import ConditionalDiagonalNormal, StandardNormal\n",
        "from nflows.transforms.base import CompositeTransform\n",
        "from nflows.transforms.autoregressive import MaskedAffineAutoregressiveTransform, MaskedPiecewiseQuadraticAutoregressiveTransform, MaskedPiecewiseLinearAutoregressiveTransform\n",
        "from nflows.transforms.permutations import ReversePermutation\n",
        "from nflows.transforms.autoregressive import MaskedUMNNAutoregressiveTransform, MaskedPiecewiseRationalQuadraticAutoregressiveTransform\n",
        "#from nflows.transforms.nonlinearities import Logit\n",
        "from nflows.nn.nets import ResidualNet\n",
        "from nflows.transforms.base import (\n",
        "    CompositeTransform,\n",
        "    InputOutsideDomain,\n",
        "    InverseTransform,\n",
        "    Transform,\n",
        ")\n",
        "\n",
        "\n",
        "from nflows.transforms.autoregressive import AutoregressiveTransform\n",
        "from nflows.transforms import made as made_module\n",
        "from nflows.utils import torchutils\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "-m6Dexkgk8ht"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "eILRGWFulNQT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn.datasets as datasets\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import copy"
      ],
      "metadata": {
        "id": "qv90qp4plOiu"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fill_ndarray(t1):\n",
        "    for i in range(t1.shape[1]):  \n",
        "        temp_col = t1[:, i]  \n",
        "        nan_num = np.count_nonzero(temp_col != temp_col)\n",
        "        if nan_num != 0:  \n",
        "            temp_not_nan_col = temp_col[temp_col == temp_col]  \n",
        "            temp_col[np.isnan(temp_col)] = temp_not_nan_col.mean()  \n",
        "    return t1\n",
        "\n",
        "def get_data(df):\n",
        "  data = df['TARGETVAR'].values.reshape((-1,1))\n",
        "  return data\n",
        "\n",
        "def get_nwp(d):\n",
        "  cls = d.columns\n",
        "  data = []\n",
        "  for i in range(4):\n",
        "    data.append(d[cls[i+3]].values.reshape((-1,1)))\n",
        "  data = np.hstack(data)\n",
        "  return data\n",
        "     "
      ],
      "metadata": {
        "id": "mIQ6e0jylQDE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data = pd.read_csv('/content/Conditional-normalizing-flow-for-wind-power-forecasting/Task15_W_Zone1.csv',delimiter=',')\n",
        "\n",
        "power = get_data(data); x = np.float32(get_nwp(data))\n",
        "y = np.float32(fill_ndarray(power))"
      ],
      "metadata": {
        "id": "M9fxq5xYlSIo"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "power.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaXx6iH7nk4g",
        "outputId": "0248128f-d422-4cd5-cf72-a9b21434100e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16800, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-uw1pKwknlyc",
        "outputId": "6c79a8c3-540b-4e5e-fa26-9a0e6605d256"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16800, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = torch.tensor(y).float()\n",
        "x = torch.tensor(x).float()"
      ],
      "metadata": {
        "id": "PV2iAj2SlXbI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import math\n",
        "\n",
        "index = math.floor(x.shape[0]*0.8)\n",
        "\n",
        "y_train = y[:index]\n",
        "x_train = x[:index,:]\n",
        "\n",
        "y_test = y[index:]\n",
        "x_test = x[index:,:]\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  x_train, y_train, x_test, y_test = x_train.cuda(), y_train.cuda(), x_test.cuda(), y_test.cuda()"
      ],
      "metadata": {
        "id": "X1n3vt-al9NB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(x_train.shape,y_train.shape,x_test.shape,y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvJu3K3Tl-6K",
        "outputId": "f2af1b0d-f93c-42f8-adc5-8eab50893151"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([13440, 4]) torch.Size([13440, 1]) torch.Size([3360, 4]) torch.Size([3360, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class Sigmoid(Transform):\n",
        "    def __init__(self, temperature=1, eps=1e-6, learn_temperature=False):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        if learn_temperature:\n",
        "            self.temperature = nn.Parameter(torch.Tensor([temperature]))\n",
        "        else:\n",
        "            self.temperature = torch.Tensor([temperature]).to(device)\n",
        "\n",
        "    def forward(self, inputs, context=None):\n",
        "        inputs = self.temperature * inputs\n",
        "        outputs = torch.sigmoid(inputs)\n",
        "        logabsdet = torchutils.sum_except_batch(\n",
        "            torch.log(self.temperature) - F.softplus(-inputs) - F.softplus(inputs)\n",
        "        )\n",
        "        return outputs, logabsdet\n",
        "\n",
        "    def inverse(self, inputs, context=None):\n",
        "        if torch.min(inputs) < 0 or torch.max(inputs) > 1:\n",
        "            raise InputOutsideDomain()\n",
        "\n",
        "        inputs = torch.clamp(inputs, self.eps, 1 - self.eps).to(device)\n",
        "\n",
        "        outputs = (1 / self.temperature) * (torch.log(inputs) - torch.log1p(-inputs))\n",
        "        logabsdet = -torchutils.sum_except_batch(\n",
        "            torch.log(self.temperature)\n",
        "            - F.softplus(-self.temperature * outputs)\n",
        "            - F.softplus(self.temperature * outputs)\n",
        "        )\n",
        "        return outputs, logabsdet\n",
        "\n",
        "\n",
        "class Logit(InverseTransform):\n",
        "    def __init__(self, temperature=1, eps=1e-6):\n",
        "        super().__init__(Sigmoid(temperature=temperature, eps=eps))\n",
        "\n",
        "     "
      ],
      "metadata": {
        "id": "za9gNuhvmBah"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from nflows.transforms.base import InputOutsideDomain\n",
        "from nflows.utils import torchutils\n",
        "\n",
        "\n",
        "def unconstrained_linear_spline(\n",
        "    inputs, unnormalized_pdf, inverse=False, tail_bound=1.0, tails=\"linear\"\n",
        "):\n",
        "    inside_interval_mask = (inputs >= -tail_bound) & (inputs <= tail_bound)\n",
        "    outside_interval_mask = ~inside_interval_mask\n",
        "\n",
        "    outputs = torch.zeros_like(inputs)\n",
        "    logabsdet = torch.zeros_like(inputs)\n",
        "\n",
        "    if tails == \"linear\":\n",
        "        outputs[outside_interval_mask] = inputs[outside_interval_mask]\n",
        "        logabsdet[outside_interval_mask] = 0\n",
        "    else:\n",
        "        raise RuntimeError(\"{} tails are not implemented.\".format(tails))\n",
        "\n",
        "    if torch.any(inside_interval_mask):\n",
        "        outputs[inside_interval_mask], logabsdet[inside_interval_mask] = linear_spline(\n",
        "            inputs=inputs[inside_interval_mask],\n",
        "            unnormalized_pdf=unnormalized_pdf[inside_interval_mask, :],\n",
        "            inverse=inverse,\n",
        "            left=-tail_bound,\n",
        "            right=tail_bound,\n",
        "            bottom=-tail_bound,\n",
        "            top=tail_bound,\n",
        "        )\n",
        "\n",
        "    return outputs, logabsdet\n",
        "     \n"
      ],
      "metadata": {
        "id": "waDbtNvymF_C"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_spline(\n",
        "    inputs, unnormalized_pdf, inverse=False, left=0.0, right=1.0, bottom=0.0, top=1.0\n",
        "):\n",
        "    \"\"\"\n",
        "    Reference:\n",
        "    > MÃ¼ller et al., Neural Importance Sampling, arXiv:1808.03856, 2018.\n",
        "    \"\"\"\n",
        "    if torch.min(inputs) < left or torch.max(inputs) > right:\n",
        "        raise InputOutsideDomain()\n",
        "\n",
        "    if inverse:\n",
        "        inputs = (inputs - bottom) / (top - bottom)\n",
        "    else:\n",
        "        inputs = (inputs - left) / (right - left)\n",
        "\n",
        "    num_bins = unnormalized_pdf.size(-1)\n",
        "\n",
        "    pdf = F.softmax(unnormalized_pdf, dim=-1)\n",
        "\n",
        "    cdf = torch.cumsum(pdf, dim=-1)\n",
        "    cdf[..., -1] = 1.0\n",
        "    cdf = F.pad(cdf, pad=(1, 0), mode=\"constant\", value=0.0)\n",
        "\n",
        "    if inverse:\n",
        "        inv_bin_idx = torchutils.searchsorted(cdf, inputs)\n",
        "\n",
        "        bin_boundaries = (\n",
        "            torch.linspace(0, 1, num_bins + 1)\n",
        "            .view([1] * inputs.dim() + [-1])\n",
        "            .expand(*inputs.shape, -1)\n",
        "        ).to(device)\n",
        "\n",
        "        slopes = (cdf[..., 1:] - cdf[..., :-1]) / (\n",
        "            bin_boundaries[..., 1:] - bin_boundaries[..., :-1]\n",
        "        )\n",
        "        offsets = cdf[..., 1:] - slopes * bin_boundaries[..., 1:]\n",
        "\n",
        "        inv_bin_idx = inv_bin_idx.unsqueeze(-1)\n",
        "        input_slopes = slopes.gather(-1, inv_bin_idx)[..., 0]\n",
        "        input_offsets = offsets.gather(-1, inv_bin_idx)[..., 0]\n",
        "\n",
        "        outputs = (inputs - input_offsets) / input_slopes\n",
        "        outputs = torch.clamp(outputs, 0, 1)\n",
        "\n",
        "        logabsdet = -torch.log(input_slopes)\n",
        "    else:\n",
        "        bin_pos = inputs * num_bins\n",
        "\n",
        "        bin_idx = torch.floor(bin_pos).long()\n",
        "        bin_idx[bin_idx >= num_bins] = num_bins - 1\n",
        "\n",
        "        alpha = bin_pos - bin_idx.float()\n",
        "\n",
        "        input_pdfs = pdf.gather(-1, bin_idx[..., None])[..., 0]\n",
        "\n",
        "        outputs = cdf.gather(-1, bin_idx[..., None])[..., 0]\n",
        "        outputs += alpha * input_pdfs\n",
        "        outputs = torch.clamp(outputs, 0, 1)\n",
        "\n",
        "        bin_width = 1.0 / num_bins\n",
        "        logabsdet = torch.log(input_pdfs) - np.log(bin_width)\n",
        "\n",
        "    if inverse:\n",
        "        outputs = outputs * (right - left) + left\n",
        "    else:\n",
        "        outputs = outputs * (top - bottom) + bottom\n",
        "\n",
        "    return outputs, logabsdet"
      ],
      "metadata": {
        "id": "5Tl9L-L-mH9l"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MaskedUnconstrainedPiecewiseLinearAutoregressiveTransform(AutoregressiveTransform):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_bins,\n",
        "        features,\n",
        "        hidden_features,\n",
        "        context_features=None,\n",
        "        num_blocks=2,\n",
        "        use_residual_blocks=True,\n",
        "        random_mask=False,\n",
        "        activation=F.relu,\n",
        "        dropout_probability=0.0,\n",
        "        use_batch_norm=False,\n",
        "        tail_bound=1.0,\n",
        "        tail = 'linear',\n",
        "    ):\n",
        "        self.num_bins = num_bins\n",
        "        self.features = features\n",
        "        self.tail_bound = tail_bound\n",
        "        self.tail = tail\n",
        "        made = made_module.MADE(\n",
        "            features=features,\n",
        "            hidden_features=hidden_features,\n",
        "            context_features=context_features,\n",
        "            num_blocks=num_blocks,\n",
        "            output_multiplier=self._output_dim_multiplier(),\n",
        "            use_residual_blocks=use_residual_blocks,\n",
        "            random_mask=random_mask,\n",
        "            activation=activation,\n",
        "            dropout_probability=dropout_probability,\n",
        "            use_batch_norm=use_batch_norm,\n",
        "        )\n",
        "        super().__init__(made)\n",
        "\n",
        "    def _output_dim_multiplier(self):\n",
        "        return self.num_bins\n",
        "\n",
        "    def _elementwise(self, inputs, autoregressive_params, inverse=False):\n",
        "        batch_size = inputs.shape[0]\n",
        "\n",
        "        unnormalized_pdf = autoregressive_params.view(\n",
        "            batch_size, self.features, self._output_dim_multiplier()\n",
        "        )\n",
        "        \n",
        "        outputs, logabsdet = unconstrained_linear_spline(\n",
        "            inputs=inputs, unnormalized_pdf=unnormalized_pdf, inverse=inverse, tail_bound=self.tail_bound, tails=self.tail\n",
        "        )\n",
        "\n",
        "        return outputs, torchutils.sum_except_batch(logabsdet)\n",
        "\n",
        "    def _elementwise_forward(self, inputs, autoregressive_params):\n",
        "        return self._elementwise(inputs, autoregressive_params)\n",
        "\n",
        "    def _elementwise_inverse(self, inputs, autoregressive_params):\n",
        "        return self._elementwise(inputs, autoregressive_params, inverse=True)"
      ],
      "metadata": {
        "id": "bUvpUeVwmNZn"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nflows.transforms.splines.cubic import unconstrained_cubic_spline\n",
        "\n",
        "class MaskedUnconstrainedPiecewiseCubicAutoregressiveTransform(AutoregressiveTransform):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_bins,\n",
        "        features,\n",
        "        hidden_features,\n",
        "        context_features=None,\n",
        "        num_blocks=2,\n",
        "        use_residual_blocks=True,\n",
        "        random_mask=False,\n",
        "        activation=F.relu,\n",
        "        dropout_probability=0.0,\n",
        "        use_batch_norm=False,\n",
        "    ):\n",
        "        self.num_bins = num_bins\n",
        "        self.features = features\n",
        "        made = made_module.MADE(\n",
        "            features=features,\n",
        "            hidden_features=hidden_features,\n",
        "            context_features=context_features,\n",
        "            num_blocks=num_blocks,\n",
        "            output_multiplier=self._output_dim_multiplier(),\n",
        "            use_residual_blocks=use_residual_blocks,\n",
        "            random_mask=random_mask,\n",
        "            activation=activation,\n",
        "            dropout_probability=dropout_probability,\n",
        "            use_batch_norm=use_batch_norm,\n",
        "        )\n",
        "        super(MaskedUnconstrainedPiecewiseCubicAutoregressiveTransform, self).__init__(made)\n",
        "\n",
        "    def _output_dim_multiplier(self):\n",
        "        return self.num_bins * 2 + 2\n",
        "\n",
        "    def _elementwise(self, inputs, autoregressive_params, inverse=False):\n",
        "        batch_size = inputs.shape[0]\n",
        "\n",
        "        transform_params = autoregressive_params.view(\n",
        "            batch_size, self.features, self.num_bins * 2 + 2\n",
        "        )\n",
        "\n",
        "        unnormalized_widths = transform_params[..., : self.num_bins]\n",
        "        unnormalized_heights = transform_params[..., self.num_bins : 2 * self.num_bins]\n",
        "        derivatives = transform_params[..., 2 * self.num_bins :]\n",
        "        unnorm_derivatives_left = derivatives[..., 0][..., None]\n",
        "        unnorm_derivatives_right = derivatives[..., 1][..., None]\n",
        "\n",
        "        if hasattr(self.autoregressive_net, \"hidden_features\"):\n",
        "            unnormalized_widths /= np.sqrt(self.autoregressive_net.hidden_features)\n",
        "            unnormalized_heights /= np.sqrt(self.autoregressive_net.hidden_features)\n",
        "\n",
        "        outputs, logabsdet = unconstrained_cubic_spline(\n",
        "            inputs=inputs,\n",
        "            unnormalized_widths=unnormalized_widths,\n",
        "            unnormalized_heights=unnormalized_heights,\n",
        "            unnorm_derivatives_left=unnorm_derivatives_left,\n",
        "            unnorm_derivatives_right=unnorm_derivatives_right,\n",
        "            inverse=inverse,\n",
        "            tail_bound=1.0,\n",
        "            tails=\"linear\",\n",
        "        )\n",
        "        return outputs, torchutils.sum_except_batch(logabsdet)\n",
        "\n",
        "    def _elementwise_forward(self, inputs, autoregressive_params):\n",
        "        return self._elementwise(inputs, autoregressive_params)\n",
        "\n",
        "    def _elementwise_inverse(self, inputs, autoregressive_params):\n",
        "        return self._elementwise(inputs, autoregressive_params, inverse=True)\n",
        "     "
      ],
      "metadata": {
        "id": "7LTT_w1TmRcR"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Sequential(\n",
        "          nn.Linear(4, 512),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(512, 512),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(512, 2)\n",
        "        )\n",
        "     "
      ],
      "metadata": {
        "id": "RtUM4XBYmUb8"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_layers = 5\n",
        "base_dist = ConditionalDiagonalNormal(shape=[1],context_encoder=model)\n",
        "#base_dist = StandardNormal(shape=[1])\n",
        "\n",
        "transforms = []\n",
        "\n",
        "#transforms.append(Logit(eps=1e-3))\n",
        "#logit needs a cuda version\n",
        "for _ in range(num_layers):\n",
        "    transforms.append(ReversePermutation(features=1))\n",
        "    #transforms.append(MaskedPiecewiseRationalQuadraticAutoregressiveTransform(features=1,hidden_features=64,context_features=4,num_bins=10,tails = \"linear\"))\n",
        "    #transforms.append(MaskedPiecewiseQuadraticAutoregressiveTransform(features=1,hidden_features=256,context_features=4,num_bins=10,tails = \"linear\"))\n",
        "    #transforms.append(MaskedAffineAutoregressiveTransform(features=1,hidden_features=256,context_features=4))\n",
        "    #transforms.append(MaskedUMNNAutoregressiveTransform(features=1,hidden_features=512,context_features=4,integrand_net_layers=[256, 256, 256]))\n",
        "    #transforms.append(MaskedUnconstrainedPiecewiseLinearAutoregressiveTransform(features=1,hidden_features=256,context_features=4,num_bins=10,tail_bound=1.0,tail = 'linear'))\n",
        "    transforms.append(MaskedUnconstrainedPiecewiseCubicAutoregressiveTransform(features=1,hidden_features=256,context_features=4,num_bins=10))\n",
        "\n",
        "transform = CompositeTransform(transforms)\n",
        "\n",
        "flow = Flow(transform, base_dist)\n",
        "if torch.cuda.is_available():\n",
        "  flow = flow.cuda()\n",
        "\n",
        "#optimizer = optim.Adam(flow.parameters(),lr=1e-3)\n",
        "optimizer = optim.Adam(flow.parameters(),lr=1e-4)\n",
        "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[300,600], gamma=0.3)"
      ],
      "metadata": {
        "id": "Z6r2uHNQmWDf"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_iter = 1000\n",
        "for i in range(num_iter):\n",
        "    optimizer.zero_grad()\n",
        "    loss = -flow.log_prob(inputs=y_train, context=x_train).mean()\n",
        "    if i%10 == 0:\n",
        "      print('iteration',i,':',loss.item())\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zbwej6Txmgox",
        "outputId": "2eaf370a-51be-46e1-98dd-618f31bac2d1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 0 : 4.480475902557373\n",
            "iteration 10 : 0.481362521648407\n",
            "iteration 20 : -0.20111586153507233\n",
            "iteration 30 : -0.39565396308898926\n",
            "iteration 40 : -0.5149353742599487\n",
            "iteration 50 : -0.4903756380081177\n",
            "iteration 60 : -0.563126802444458\n",
            "iteration 70 : -0.6017073392868042\n",
            "iteration 80 : -0.6897176504135132\n",
            "iteration 90 : -0.7085854411125183\n",
            "iteration 100 : -0.6676489114761353\n",
            "iteration 110 : -0.7365836501121521\n",
            "iteration 120 : -0.743061363697052\n",
            "iteration 130 : -0.6964942812919617\n",
            "iteration 140 : -0.689651370048523\n",
            "iteration 150 : -0.7512606978416443\n",
            "iteration 160 : -0.7919875383377075\n",
            "iteration 170 : -0.44511619210243225\n",
            "iteration 180 : -0.6521691083908081\n",
            "iteration 190 : -0.6865114569664001\n",
            "iteration 200 : -0.7506049275398254\n",
            "iteration 210 : -0.7681019902229309\n",
            "iteration 220 : -0.7359240055084229\n",
            "iteration 230 : -0.7307777404785156\n",
            "iteration 240 : -0.6892122030258179\n",
            "iteration 250 : -0.7821463942527771\n",
            "iteration 260 : -0.8055863380432129\n",
            "iteration 270 : -0.7495648860931396\n",
            "iteration 280 : -0.8079308271408081\n",
            "iteration 290 : -0.705657958984375\n",
            "iteration 300 : -0.7991487383842468\n",
            "iteration 310 : -0.8359254002571106\n",
            "iteration 320 : -0.8519846200942993\n",
            "iteration 330 : -0.8587206602096558\n",
            "iteration 340 : -0.8642157316207886\n",
            "iteration 350 : -0.8682674169540405\n",
            "iteration 360 : -0.8709760904312134\n",
            "iteration 370 : -0.8751075863838196\n",
            "iteration 380 : -0.8782808780670166\n",
            "iteration 390 : -0.7247281074523926\n",
            "iteration 400 : -0.8426614999771118\n",
            "iteration 410 : -0.8591827750205994\n",
            "iteration 420 : -0.8768810629844666\n",
            "iteration 430 : -0.8836048245429993\n",
            "iteration 440 : -0.885692298412323\n",
            "iteration 450 : -0.810914158821106\n",
            "iteration 460 : -0.8621184825897217\n",
            "iteration 470 : -0.8818740248680115\n",
            "iteration 480 : -0.7827894687652588\n",
            "iteration 490 : -0.8477747440338135\n",
            "iteration 500 : -0.8858503699302673\n",
            "iteration 510 : -0.8370140790939331\n",
            "iteration 520 : -0.8775808215141296\n",
            "iteration 530 : -0.7236078381538391\n",
            "iteration 540 : -0.8122153282165527\n",
            "iteration 550 : -0.8618821501731873\n",
            "iteration 560 : -0.8865708708763123\n",
            "iteration 570 : -0.8088467717170715\n",
            "iteration 580 : -0.8640976548194885\n",
            "iteration 590 : -0.9000247120857239\n",
            "iteration 600 : -0.9037342071533203\n",
            "iteration 610 : -0.910345733165741\n",
            "iteration 620 : -0.9122961759567261\n",
            "iteration 630 : -0.9137134552001953\n",
            "iteration 640 : -0.9149429202079773\n",
            "iteration 650 : -0.9160940051078796\n",
            "iteration 660 : -0.9171728491783142\n",
            "iteration 670 : -0.9181287884712219\n",
            "iteration 680 : -0.9190539121627808\n",
            "iteration 690 : -0.9199565649032593\n",
            "iteration 700 : -0.9207971692085266\n",
            "iteration 710 : -0.9215450286865234\n",
            "iteration 720 : -0.9223970174789429\n",
            "iteration 730 : -0.9233099222183228\n",
            "iteration 740 : -0.9240063428878784\n",
            "iteration 750 : -0.9248164892196655\n",
            "iteration 760 : -0.9256265163421631\n",
            "iteration 770 : -0.9262863993644714\n",
            "iteration 780 : -0.9267525672912598\n",
            "iteration 790 : -0.9271805882453918\n",
            "iteration 800 : -0.9097145199775696\n",
            "iteration 810 : -0.9164016246795654\n",
            "iteration 820 : -0.9099507331848145\n",
            "iteration 830 : -0.9246310591697693\n",
            "iteration 840 : -0.9272648096084595\n",
            "iteration 850 : -0.928771436214447\n",
            "iteration 860 : -0.9124230146408081\n",
            "iteration 870 : -0.8539522886276245\n",
            "iteration 880 : -0.8981780409812927\n",
            "iteration 890 : -0.9185670614242554\n",
            "iteration 900 : -0.9285358786582947\n",
            "iteration 910 : -0.9309306740760803\n",
            "iteration 920 : -0.9307374358177185\n",
            "iteration 930 : -0.9316911101341248\n",
            "iteration 940 : -0.9346431493759155\n",
            "iteration 950 : -0.9351875185966492\n",
            "iteration 960 : -0.9341462850570679\n",
            "iteration 970 : -0.9322319626808167\n",
            "iteration 980 : -0.9313303828239441\n",
            "iteration 990 : -0.9332775473594666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import properscoring as ps\n",
        "y_true = y_test.cpu().numpy()"
      ],
      "metadata": {
        "id": "pp2i_RywmnSM"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "score = 0\n",
        "samples = []\n",
        "with torch.no_grad():\n",
        "  for i in range(y_true.shape[0]):\n",
        "    sample = flow.sample(1000,context=x_test[i,:].reshape((-1,4))).cpu().numpy()\n",
        "    sample = sample.squeeze()\n",
        "    score += ps.crps_ensemble(y_true[i,0] , sample)\n",
        "    samples.append(list(sample))\n",
        "samples = np.array(samples)\n",
        "print(samples.shape)\n",
        "print(score/y_true.shape[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mV2_u9lFnFEB",
        "outputId": "fbe45db8-231a-460c-81de-755516d7a00d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3360, 1000)\n",
            "0.09139957400376703\n"
          ]
        }
      ]
    }
  ]
}