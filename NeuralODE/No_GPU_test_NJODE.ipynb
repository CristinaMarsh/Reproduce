{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "No-GPU test NJODE.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOrEA71lOsFW+TaHH2xy+wU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CristinaMarsh/Reproduce/blob/main/NeuralODE/No_GPU_test_NJODE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thanks: https://github.com/HerreraKrachTeichmann/NJODE.git\n",
        "\n",
        "\n",
        "I haven't attempted run in GPU"
      ],
      "metadata": {
        "id": "HZT5i_JFJxvl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZ-btF72F3Jj",
        "outputId": "e1108b45-a9db-4123-a684-0faf15ffc154"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'NJODE'...\n",
            "remote: Enumerating objects: 176, done.\u001b[K\n",
            "remote: Counting objects: 100% (176/176), done.\u001b[K\n",
            "remote: Compressing objects: 100% (137/137), done.\u001b[K\n",
            "remote: Total 176 (delta 57), reused 131 (delta 28), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (176/176), 8.15 MiB | 11.15 MiB/s, done.\n",
            "Resolving deltas: 100% (57/57), done.\n"
          ]
        }
      ],
      "source": [
        "! git clone https://github.com/HerreraKrachTeichmann/NJODE.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd NJODE/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVYvyMR8GH_5",
        "outputId": "e6c5c0a8-6844-4b6e-804f-c8efb23de5c7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/NJODE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_11-IBp5F_DX",
        "outputId": "ec15a22b-baa7-4e4a-8558-7a3ee0d3f2b0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting absl-py==0.11.0\n",
            "  Downloading absl_py-0.11.0-py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 7.5 MB/s \n",
            "\u001b[?25hCollecting aiohttp==3.6.2\n",
            "  Downloading aiohttp-3.6.2-cp37-cp37m-manylinux1_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 53.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor==0.8.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (0.8.1)\n",
            "Collecting async-timeout==3.0.1\n",
            "  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
            "Collecting attrs==20.2.0\n",
            "  Downloading attrs-20.2.0-py2.py3-none-any.whl (48 kB)\n",
            "\u001b[K     |████████████████████████████████| 48 kB 6.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cached-property==1.5.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (1.5.2)\n",
            "Collecting cachetools==4.1.1\n",
            "  Downloading cachetools-4.1.1-py3-none-any.whl (10 kB)\n",
            "Collecting certifi==2020.4.5.1\n",
            "  Downloading certifi-2020.4.5.1-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 51.9 MB/s \n",
            "\u001b[?25hCollecting cffi==1.14.2\n",
            "  Downloading cffi-1.14.2-cp37-cp37m-manylinux1_x86_64.whl (401 kB)\n",
            "\u001b[K     |████████████████████████████████| 401 kB 51.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet==3.0.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 10)) (3.0.4)\n",
            "Collecting cryptography==3.1\n",
            "  Downloading cryptography-3.1-cp35-abi3-manylinux2010_x86_64.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 48.6 MB/s \n",
            "\u001b[?25hCollecting cycler==0.10.0\n",
            "  Downloading cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
            "Collecting dataclasses==0.6\n",
            "  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: decorator==4.4.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 14)) (4.4.2)\n",
            "Collecting future==0.18.2\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 49.9 MB/s \n",
            "\u001b[?25hCollecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Collecting google-auth==1.23.0\n",
            "  Downloading google_auth-1.23.0-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[K     |████████████████████████████████| 114 kB 70.1 MB/s \n",
            "\u001b[?25hCollecting google-auth-oauthlib==0.4.2\n",
            "  Downloading google_auth_oauthlib-0.4.2-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: google-pasta==0.2.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 19)) (0.2.0)\n",
            "Collecting grpcio==1.33.2\n",
            "  Downloading grpcio-1.33.2-cp37-cp37m-manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 45.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py==3.1.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 21)) (3.1.0)\n",
            "Requirement already satisfied: idna==2.10 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 22)) (2.10)\n",
            "Collecting imageio==2.8.0\n",
            "  Downloading imageio-2.8.0-py3-none-any.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 39.3 MB/s \n",
            "\u001b[?25hCollecting importlib-metadata==2.0.0\n",
            "  Downloading importlib_metadata-2.0.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting joblib==0.15.1\n",
            "  Downloading joblib-0.15.1-py3-none-any.whl (298 kB)\n",
            "\u001b[K     |████████████████████████████████| 298 kB 60.6 MB/s \n",
            "\u001b[?25hCollecting Keras-Applications==1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 6.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Keras-Preprocessing==1.1.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 27)) (1.1.2)\n",
            "Collecting kiwisolver==1.2.0\n",
            "  Downloading kiwisolver-1.2.0-cp37-cp37m-manylinux1_x86_64.whl (88 kB)\n",
            "\u001b[K     |████████████████████████████████| 88 kB 9.7 MB/s \n",
            "\u001b[?25hCollecting Markdown==3.3.3\n",
            "  Downloading Markdown-3.3.3-py3-none-any.whl (96 kB)\n",
            "\u001b[K     |████████████████████████████████| 96 kB 6.7 MB/s \n",
            "\u001b[?25hCollecting matplotlib==3.2.1\n",
            "  Downloading matplotlib-3.2.1-cp37-cp37m-manylinux1_x86_64.whl (12.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.4 MB 30.7 MB/s \n",
            "\u001b[?25hCollecting multidict==4.7.6\n",
            "  Downloading multidict-4.7.6-cp37-cp37m-manylinux1_x86_64.whl (149 kB)\n",
            "\u001b[K     |████████████████████████████████| 149 kB 58.8 MB/s \n",
            "\u001b[?25hCollecting numpy==1.18.5\n",
            "  Downloading numpy-1.18.5-cp37-cp37m-manylinux1_x86_64.whl (20.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 20.1 MB 1.1 MB/s \n",
            "\u001b[?25hCollecting oauthlib==3.1.0\n",
            "  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
            "\u001b[K     |████████████████████████████████| 147 kB 61.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum==3.3.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 34)) (3.3.0)\n",
            "Collecting pandas==1.0.4\n",
            "  Downloading pandas-1.0.4-cp37-cp37m-manylinux1_x86_64.whl (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 27.4 MB/s \n",
            "\u001b[?25hCollecting pdf2image==1.13.1\n",
            "  Downloading pdf2image-1.13.1-py3-none-any.whl (10.0 kB)\n",
            "Requirement already satisfied: Pillow==7.1.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 37)) (7.1.2)\n",
            "Collecting protobuf==3.14.0\n",
            "  Downloading protobuf-3.14.0-cp37-cp37m-manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 52.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyasn1==0.4.8 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 39)) (0.4.8)\n",
            "Requirement already satisfied: pyasn1-modules==0.2.8 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 40)) (0.2.8)\n",
            "Collecting pycparser==2.20\n",
            "  Downloading pycparser-2.20-py2.py3-none-any.whl (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 44.4 MB/s \n",
            "\u001b[?25hCollecting pyparsing==2.4.7\n",
            "  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 7.0 MB/s \n",
            "\u001b[?25hCollecting python-dateutil==2.8.1\n",
            "  Downloading python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB)\n",
            "\u001b[K     |████████████████████████████████| 227 kB 60.5 MB/s \n",
            "\u001b[?25hCollecting python-telegram-bot==12.8\n",
            "  Downloading python_telegram_bot-12.8-py2.py3-none-any.whl (375 kB)\n",
            "\u001b[K     |████████████████████████████████| 375 kB 48.9 MB/s \n",
            "\u001b[?25hCollecting pytz==2020.1\n",
            "  Downloading pytz-2020.1-py2.py3-none-any.whl (510 kB)\n",
            "\u001b[K     |████████████████████████████████| 510 kB 57.5 MB/s \n",
            "\u001b[?25hCollecting requests==2.24.0\n",
            "  Downloading requests-2.24.0-py2.py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 470 kB/s \n",
            "\u001b[?25hCollecting requests-oauthlib==1.3.0\n",
            "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
            "Collecting rsa==4.6\n",
            "  Downloading rsa-4.6-py3-none-any.whl (47 kB)\n",
            "\u001b[K     |████████████████████████████████| 47 kB 5.3 MB/s \n",
            "\u001b[?25hCollecting scikit-learn==0.23.1\n",
            "  Downloading scikit_learn-0.23.1-cp37-cp37m-manylinux1_x86_64.whl (6.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8 MB 41.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 50)) (1.4.1)\n",
            "Requirement already satisfied: six==1.15.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 51)) (1.15.0)\n",
            "Requirement already satisfied: sklearn==0.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 52)) (0.0)\n",
            "Collecting tensorboard==1.15.0\n",
            "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 37.5 MB/s \n",
            "\u001b[?25hCollecting tensorboard-plugin-wit==1.7.0\n",
            "  Downloading tensorboard_plugin_wit-1.7.0-py3-none-any.whl (779 kB)\n",
            "\u001b[K     |████████████████████████████████| 779 kB 73.7 MB/s \n",
            "\u001b[?25hCollecting tensorflow==1.15.0\n",
            "  Downloading tensorflow-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (412.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 412.3 MB 25 kB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator==1.15.1\n",
            "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[K     |████████████████████████████████| 503 kB 62.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor==1.1.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 57)) (1.1.0)\n",
            "Collecting threadpoolctl==2.1.0\n",
            "  Downloading threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\n",
            "Collecting torch==1.5.0\n",
            "  Downloading torch-1.5.0-cp37-cp37m-manylinux1_x86_64.whl (752.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 752.0 MB 9.9 kB/s \n",
            "\u001b[?25hCollecting torchdiffeq==0.1.1\n",
            "  Downloading torchdiffeq-0.1.1-py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: torchsummary==1.5.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 61)) (1.5.1)\n",
            "Collecting torchvision==0.6.0\n",
            "  Downloading torchvision-0.6.0-cp37-cp37m-manylinux1_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 39.6 MB/s \n",
            "\u001b[?25hCollecting tornado==6.0.4\n",
            "  Downloading tornado-6.0.4.tar.gz (496 kB)\n",
            "\u001b[K     |████████████████████████████████| 496 kB 70.0 MB/s \n",
            "\u001b[?25hCollecting tqdm==4.46.1\n",
            "  Downloading tqdm-4.46.1-py2.py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.6 MB/s \n",
            "\u001b[?25hCollecting typing-extensions==3.7.4.3\n",
            "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
            "Collecting urllib3==1.25.10\n",
            "  Downloading urllib3-1.25.10-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 68.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Werkzeug==1.0.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 67)) (1.0.1)\n",
            "Collecting wrapt==1.12.1\n",
            "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
            "Collecting yarl==1.5.1\n",
            "  Downloading yarl-1.5.1-cp37-cp37m-manylinux1_x86_64.whl (258 kB)\n",
            "\u001b[K     |████████████████████████████████| 258 kB 60.6 MB/s \n",
            "\u001b[?25hCollecting zipp==3.4.0\n",
            "  Downloading zipp-3.4.0-py3-none-any.whl (5.2 kB)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-auth==1.23.0->-r requirements.txt (line 17)) (57.4.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard==1.15.0->-r requirements.txt (line 53)) (0.37.1)\n",
            "Building wheels for collected packages: future, gast, tornado, wrapt\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=0e54391773729cebe4dbe06c5bb7785e289016dbd9cbfcb0ec51dd2ed7b8f752\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=79f95bb3f1558f3ff1d789b1f45afd41a494ea905f3dc7119c2a4b5b79bc50af\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "  Building wheel for tornado (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tornado: filename=tornado-6.0.4-cp37-cp37m-linux_x86_64.whl size=428600 sha256=a1598a61ad5571c5976782e16cd000ed21b6f2fa905e8f04f533c01255eaf221\n",
            "  Stored in directory: /root/.cache/pip/wheels/7d/14/fa/d88fb5da77d813ea0ffca38a2ab2a052874e9e1142bad0b348\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wrapt: filename=wrapt-1.12.1-cp37-cp37m-linux_x86_64.whl size=68721 sha256=b5575aeff5b139ba254dd70f04b64bee2ba78b4171cb3862e7a622ad71920fc7\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/76/4c/aa25851149f3f6d9785f6c869387ad82b3fd37582fa8147ac6\n",
            "Successfully built future gast tornado wrapt\n",
            "Installing collected packages: zipp, urllib3, pycparser, numpy, importlib-metadata, certifi, typing-extensions, threadpoolctl, rsa, requests, protobuf, oauthlib, multidict, Markdown, joblib, grpcio, future, cffi, cachetools, absl-py, yarl, wrapt, tornado, torch, tensorflow-estimator, tensorboard, scikit-learn, requests-oauthlib, pytz, python-dateutil, pyparsing, kiwisolver, Keras-Applications, google-auth, gast, cycler, cryptography, attrs, async-timeout, tqdm, torchvision, torchdiffeq, tensorflow, tensorboard-plugin-wit, python-telegram-bot, pdf2image, pandas, matplotlib, imageio, google-auth-oauthlib, dataclasses, aiohttp\n",
            "  Attempting uninstall: zipp\n",
            "    Found existing installation: zipp 3.8.0\n",
            "    Uninstalling zipp-3.8.0:\n",
            "      Successfully uninstalled zipp-3.8.0\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: pycparser\n",
            "    Found existing installation: pycparser 2.21\n",
            "    Uninstalling pycparser-2.21:\n",
            "      Successfully uninstalled pycparser-2.21\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 4.11.3\n",
            "    Uninstalling importlib-metadata-4.11.3:\n",
            "      Successfully uninstalled importlib-metadata-4.11.3\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2022.5.18.1\n",
            "    Uninstalling certifi-2022.5.18.1:\n",
            "      Successfully uninstalled certifi-2022.5.18.1\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 4.2.0\n",
            "    Uninstalling typing-extensions-4.2.0:\n",
            "      Successfully uninstalled typing-extensions-4.2.0\n",
            "  Attempting uninstall: threadpoolctl\n",
            "    Found existing installation: threadpoolctl 3.1.0\n",
            "    Uninstalling threadpoolctl-3.1.0:\n",
            "      Successfully uninstalled threadpoolctl-3.1.0\n",
            "  Attempting uninstall: rsa\n",
            "    Found existing installation: rsa 4.8\n",
            "    Uninstalling rsa-4.8:\n",
            "      Successfully uninstalled rsa-4.8\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.17.3\n",
            "    Uninstalling protobuf-3.17.3:\n",
            "      Successfully uninstalled protobuf-3.17.3\n",
            "  Attempting uninstall: oauthlib\n",
            "    Found existing installation: oauthlib 3.2.0\n",
            "    Uninstalling oauthlib-3.2.0:\n",
            "      Successfully uninstalled oauthlib-3.2.0\n",
            "  Attempting uninstall: Markdown\n",
            "    Found existing installation: Markdown 3.3.7\n",
            "    Uninstalling Markdown-3.3.7:\n",
            "      Successfully uninstalled Markdown-3.3.7\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.1.0\n",
            "    Uninstalling joblib-1.1.0:\n",
            "      Successfully uninstalled joblib-1.1.0\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.46.1\n",
            "    Uninstalling grpcio-1.46.1:\n",
            "      Successfully uninstalled grpcio-1.46.1\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "  Attempting uninstall: cffi\n",
            "    Found existing installation: cffi 1.15.0\n",
            "    Uninstalling cffi-1.15.0:\n",
            "      Successfully uninstalled cffi-1.15.0\n",
            "  Attempting uninstall: cachetools\n",
            "    Found existing installation: cachetools 4.2.4\n",
            "    Uninstalling cachetools-4.2.4:\n",
            "      Successfully uninstalled cachetools-4.2.4\n",
            "  Attempting uninstall: absl-py\n",
            "    Found existing installation: absl-py 1.0.0\n",
            "    Uninstalling absl-py-1.0.0:\n",
            "      Successfully uninstalled absl-py-1.0.0\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.14.1\n",
            "    Uninstalling wrapt-1.14.1:\n",
            "      Successfully uninstalled wrapt-1.14.1\n",
            "  Attempting uninstall: tornado\n",
            "    Found existing installation: tornado 5.1.1\n",
            "    Uninstalling tornado-5.1.1:\n",
            "      Successfully uninstalled tornado-5.1.1\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.11.0+cu113\n",
            "    Uninstalling torch-1.11.0+cu113:\n",
            "      Successfully uninstalled torch-1.11.0+cu113\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.8.0\n",
            "    Uninstalling tensorboard-2.8.0:\n",
            "      Successfully uninstalled tensorboard-2.8.0\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.2\n",
            "    Uninstalling scikit-learn-1.0.2:\n",
            "      Successfully uninstalled scikit-learn-1.0.2\n",
            "  Attempting uninstall: requests-oauthlib\n",
            "    Found existing installation: requests-oauthlib 1.3.1\n",
            "    Uninstalling requests-oauthlib-1.3.1:\n",
            "      Successfully uninstalled requests-oauthlib-1.3.1\n",
            "  Attempting uninstall: pytz\n",
            "    Found existing installation: pytz 2022.1\n",
            "    Uninstalling pytz-2022.1:\n",
            "      Successfully uninstalled pytz-2022.1\n",
            "  Attempting uninstall: python-dateutil\n",
            "    Found existing installation: python-dateutil 2.8.2\n",
            "    Uninstalling python-dateutil-2.8.2:\n",
            "      Successfully uninstalled python-dateutil-2.8.2\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 3.0.9\n",
            "    Uninstalling pyparsing-3.0.9:\n",
            "      Successfully uninstalled pyparsing-3.0.9\n",
            "  Attempting uninstall: kiwisolver\n",
            "    Found existing installation: kiwisolver 1.4.2\n",
            "    Uninstalling kiwisolver-1.4.2:\n",
            "      Successfully uninstalled kiwisolver-1.4.2\n",
            "  Attempting uninstall: google-auth\n",
            "    Found existing installation: google-auth 1.35.0\n",
            "    Uninstalling google-auth-1.35.0:\n",
            "      Successfully uninstalled google-auth-1.35.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "  Attempting uninstall: cycler\n",
            "    Found existing installation: cycler 0.11.0\n",
            "    Uninstalling cycler-0.11.0:\n",
            "      Successfully uninstalled cycler-0.11.0\n",
            "  Attempting uninstall: attrs\n",
            "    Found existing installation: attrs 21.4.0\n",
            "    Uninstalling attrs-21.4.0:\n",
            "      Successfully uninstalled attrs-21.4.0\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.64.0\n",
            "    Uninstalling tqdm-4.64.0:\n",
            "      Successfully uninstalled tqdm-4.64.0\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.12.0+cu113\n",
            "    Uninstalling torchvision-0.12.0+cu113:\n",
            "      Successfully uninstalled torchvision-0.12.0+cu113\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.0+zzzcolab20220506162203\n",
            "    Uninstalling tensorflow-2.8.0+zzzcolab20220506162203:\n",
            "      Successfully uninstalled tensorflow-2.8.0+zzzcolab20220506162203\n",
            "  Attempting uninstall: tensorboard-plugin-wit\n",
            "    Found existing installation: tensorboard-plugin-wit 1.8.1\n",
            "    Uninstalling tensorboard-plugin-wit-1.8.1:\n",
            "      Successfully uninstalled tensorboard-plugin-wit-1.8.1\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.3.5\n",
            "    Uninstalling pandas-1.3.5:\n",
            "      Successfully uninstalled pandas-1.3.5\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.2.2\n",
            "    Uninstalling matplotlib-3.2.2:\n",
            "      Successfully uninstalled matplotlib-3.2.2\n",
            "  Attempting uninstall: imageio\n",
            "    Found existing installation: imageio 2.4.1\n",
            "    Uninstalling imageio-2.4.1:\n",
            "      Successfully uninstalled imageio-2.4.1\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 0.4.6\n",
            "    Uninstalling google-auth-oauthlib-0.4.6:\n",
            "      Successfully uninstalled google-auth-oauthlib-0.4.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.4 requires scikit-learn>=1.0.0, but you have scikit-learn 0.23.1 which is incompatible.\n",
            "xarray 0.20.2 requires pandas>=1.1, but you have pandas 1.0.4 which is incompatible.\n",
            "xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.18.5 which is incompatible.\n",
            "torchtext 0.12.0 requires torch==1.11.0, but you have torch 1.5.0 which is incompatible.\n",
            "torchaudio 0.11.0+cu113 requires torch==1.11.0, but you have torch 1.5.0 which is incompatible.\n",
            "tensorflow-probability 0.16.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\n",
            "tables 3.7.0 requires numpy>=1.19.0, but you have numpy 1.18.5 which is incompatible.\n",
            "pymc3 3.11.4 requires cachetools>=4.2.1, but you have cachetools 4.1.1 which is incompatible.\n",
            "pydata-google-auth 1.4.0 requires google-auth<3.0dev,>=1.25.0; python_version >= \"3.6\", but you have google-auth 1.23.0 which is incompatible.\n",
            "panel 0.12.1 requires tqdm>=4.48.0, but you have tqdm 4.46.1 which is incompatible.\n",
            "kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.15.0 which is incompatible.\n",
            "jaxlib 0.3.7+cuda11.cudnn805 requires numpy>=1.19, but you have numpy 1.18.5 which is incompatible.\n",
            "jax 0.3.8 requires numpy>=1.19, but you have numpy 1.18.5 which is incompatible.\n",
            "imbalanced-learn 0.8.1 requires scikit-learn>=0.24, but you have scikit-learn 0.23.1 which is incompatible.\n",
            "googleapis-common-protos 1.56.1 requires protobuf>=3.15.0, but you have protobuf 3.14.0 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas>=1.1.0; python_version >= \"3.0\", but you have pandas 1.0.4 which is incompatible.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.24.0 which is incompatible.\n",
            "google-colab 1.0.0 requires tornado~=5.1.0; python_version >= \"3.0\", but you have tornado 6.0.4 which is incompatible.\n",
            "google-api-core 1.31.5 requires google-auth<2.0dev,>=1.25.0, but you have google-auth 1.23.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed Keras-Applications-1.0.8 Markdown-3.3.3 absl-py-0.11.0 aiohttp-3.6.2 async-timeout-3.0.1 attrs-20.2.0 cachetools-4.1.1 certifi-2020.4.5.1 cffi-1.14.2 cryptography-3.1 cycler-0.10.0 dataclasses-0.6 future-0.18.2 gast-0.2.2 google-auth-1.23.0 google-auth-oauthlib-0.4.2 grpcio-1.33.2 imageio-2.8.0 importlib-metadata-2.0.0 joblib-0.15.1 kiwisolver-1.2.0 matplotlib-3.2.1 multidict-4.7.6 numpy-1.18.5 oauthlib-3.1.0 pandas-1.0.4 pdf2image-1.13.1 protobuf-3.14.0 pycparser-2.20 pyparsing-2.4.7 python-dateutil-2.8.1 python-telegram-bot-12.8 pytz-2020.1 requests-2.24.0 requests-oauthlib-1.3.0 rsa-4.6 scikit-learn-0.23.1 tensorboard-1.15.0 tensorboard-plugin-wit-1.7.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1 threadpoolctl-2.1.0 torch-1.5.0 torchdiffeq-0.1.1 torchvision-0.6.0 tornado-6.0.4 tqdm-4.46.1 typing-extensions-3.7.4.3 urllib3-1.25.10 wrapt-1.12.1 yarl-1.5.1 zipp-3.4.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "cffi",
                  "cycler",
                  "dateutil",
                  "google",
                  "kiwisolver",
                  "matplotlib",
                  "mpl_toolkits",
                  "numpy",
                  "pyparsing",
                  "tornado",
                  "typing_extensions"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd NJODE/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhtQsrP4HNdC",
        "outputId": "241de114-cf86-45e2-a83f-0b2f0faffea9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/NJODE/NJODE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python demo.py --dataset='BlackScholes'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vT2j4AUDGL0R",
        "outputId": "965de8a6-51d4-4c5b-c60d-4532a645478f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4b58b7ca364f\n",
            "SERVER=False\n",
            "no dataset exists for: BlackScholes -> generate dateset...\n",
            "dataset stored as: ../data/training_data/BlackScholes-1654085001/\n",
            "using loss: standard\n",
            "use residual network: input_size=1, output_size=10\n",
            "use residual network: input_size=10, output_size=1\n",
            "model-id: None\n",
            "\n",
            "using CPU\n",
            "optimal eval loss (achieved by true cond exp): 0.13727\n",
            "new model_id=4\n",
            "model params:\n",
            "{\"batch_size\": 100, \"bias\": true, \"dataset\": \"BlackScholes\", \"dataset_id\": 1654085001, \"dropout_rate\": 0.1, \"enc_nn\": [[50, \"tanh\"], [50, \"tanh\"]], \"epochs\": 200, \"hidden_size\": 10, \"input_size\": 1, \"learning_rate\": 0.001, \"ode_nn\": [[50, \"tanh\"], [50, \"tanh\"]], \"optimal_eval_loss\": 0.13727061074083916, \"options\": {\"plot_only\": false, \"residual_enc_dec\": true, \"which_loss\": \"standard\"}, \"output_size\": 1, \"readout_nn\": [[50, \"tanh\"], [50, \"tanh\"]], \"seed\": 398, \"solver\": \"euler\", \"test_size\": 0.2, \"use_rnn\": false, \"weight\": 0.5, \"weight_decay\": 1.0}\n",
            "initiate new model ...\n",
            "\n",
            "model overview:\n",
            "NJODE(\n",
            "  (ode_f): ODEFunc(\n",
            "    (f): Sequential(\n",
            "      (0): Linear(in_features=13, out_features=50, bias=True)\n",
            "      (1): Tanh()\n",
            "      (2): Dropout(p=0.1, inplace=False)\n",
            "      (3): Linear(in_features=50, out_features=50, bias=True)\n",
            "      (4): Tanh()\n",
            "      (5): Dropout(p=0.1, inplace=False)\n",
            "      (6): Linear(in_features=50, out_features=10, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (encoder_map): FFNN(\n",
            "    (ffnn): Sequential(\n",
            "      (0): Linear(in_features=1, out_features=50, bias=True)\n",
            "      (1): Tanh()\n",
            "      (2): Dropout(p=0.1, inplace=False)\n",
            "      (3): Linear(in_features=50, out_features=50, bias=True)\n",
            "      (4): Tanh()\n",
            "      (5): Dropout(p=0.1, inplace=False)\n",
            "      (6): Linear(in_features=50, out_features=10, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (readout_map): FFNN(\n",
            "    (ffnn): Sequential(\n",
            "      (0): Linear(in_features=10, out_features=50, bias=True)\n",
            "      (1): Tanh()\n",
            "      (2): Dropout(p=0.1, inplace=False)\n",
            "      (3): Linear(in_features=50, out_features=50, bias=True)\n",
            "      (4): Tanh()\n",
            "      (5): Dropout(p=0.1, inplace=False)\n",
            "      (6): Linear(in_features=50, out_features=1, bias=True)\n",
            "    )\n",
            "  )\n",
            ") \n",
            "\n",
            "# parameters=10071\n",
            "\n",
            "start training ...\n",
            "160it [00:50,  3.17it/s]\n",
            "epoch 1, weight=0.50000, train-loss=0.29228, optimal-eval-loss=0.13727, eval-loss=0.29422, \n",
            "save new best model: last-best-loss: inf, new-best-loss: 0.29422, epoch: 1\n",
            "saved!\n",
            "160it [00:51,  3.11it/s]\n",
            "epoch 2, weight=0.50000, train-loss=0.20567, optimal-eval-loss=0.13727, eval-loss=0.21052, \n",
            "save new best model: last-best-loss: 0.29422, new-best-loss: 0.21052, epoch: 2\n",
            "saved!\n",
            "160it [00:50,  3.14it/s]\n",
            "epoch 3, weight=0.50000, train-loss=0.20118, optimal-eval-loss=0.13727, eval-loss=0.18064, \n",
            "save new best model: last-best-loss: 0.21052, new-best-loss: 0.18064, epoch: 3\n",
            "saved!\n",
            "160it [00:50,  3.16it/s]\n",
            "epoch 4, weight=0.50000, train-loss=0.16983, optimal-eval-loss=0.13727, eval-loss=0.16876, \n",
            "save new best model: last-best-loss: 0.18064, new-best-loss: 0.16876, epoch: 4\n",
            "saved!\n",
            "160it [00:51,  3.13it/s]\n",
            "epoch 5, weight=0.50000, train-loss=0.17472, optimal-eval-loss=0.13727, eval-loss=0.15857, \n",
            "plotting ...\n",
            "optimal eval-loss (with current weight=0.50000): 0.13727\n",
            "save model ...\n",
            "saved!\n",
            "save new best model: last-best-loss: 0.16876, new-best-loss: 0.15857, epoch: 5\n",
            "saved!\n",
            "160it [00:51,  3.11it/s]\n",
            "epoch 6, weight=0.50000, train-loss=0.12711, optimal-eval-loss=0.13727, eval-loss=0.15501, \n",
            "save new best model: last-best-loss: 0.15857, new-best-loss: 0.15501, epoch: 6\n",
            "saved!\n",
            "45it [00:14,  3.24it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python demo.py --dataset='OrnsteinUhlenbeck'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WsHLfHXHRPb",
        "outputId": "c8f41fb8-82de-404c-91a8-4426820b8e33"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "39ba1ac7265b\n",
            "SERVER=False\n",
            "no dataset exists for: OrnsteinUhlenbeck -> generate dateset...\n",
            "dataset stored as: ../data/training_data/OrnsteinUhlenbeck-1654084624/\n",
            "using loss: standard\n",
            "use residual network: input_size=1, output_size=10\n",
            "use residual network: input_size=10, output_size=1\n",
            "model-id: None\n",
            "\n",
            "using GPU\n",
            "optimal eval loss (achieved by true cond exp): 0.00674\n",
            "new model_id=6\n",
            "model params:\n",
            "{\"batch_size\": 100, \"bias\": true, \"dataset\": \"OrnsteinUhlenbeck\", \"dataset_id\": 1654084624, \"dropout_rate\": 0.1, \"enc_nn\": [[50, \"tanh\"], [50, \"tanh\"]], \"epochs\": 200, \"hidden_size\": 10, \"input_size\": 1, \"learning_rate\": 0.001, \"ode_nn\": [[50, \"tanh\"], [50, \"tanh\"]], \"optimal_eval_loss\": 0.006736717599051658, \"options\": {\"plot_only\": false, \"residual_enc_dec\": true, \"which_loss\": \"standard\"}, \"output_size\": 1, \"readout_nn\": [[50, \"tanh\"], [50, \"tanh\"]], \"seed\": 398, \"solver\": \"euler\", \"test_size\": 0.2, \"use_rnn\": false, \"weight\": 0.5, \"weight_decay\": 1.0}\n",
            "initiate new model ...\n",
            "\n",
            "model overview:\n",
            "NJODE(\n",
            "  (ode_f): ODEFunc(\n",
            "    (f): Sequential(\n",
            "      (0): Linear(in_features=13, out_features=50, bias=True)\n",
            "      (1): Tanh()\n",
            "      (2): Dropout(p=0.1, inplace=False)\n",
            "      (3): Linear(in_features=50, out_features=50, bias=True)\n",
            "      (4): Tanh()\n",
            "      (5): Dropout(p=0.1, inplace=False)\n",
            "      (6): Linear(in_features=50, out_features=10, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (encoder_map): FFNN(\n",
            "    (ffnn): Sequential(\n",
            "      (0): Linear(in_features=1, out_features=50, bias=True)\n",
            "      (1): Tanh()\n",
            "      (2): Dropout(p=0.1, inplace=False)\n",
            "      (3): Linear(in_features=50, out_features=50, bias=True)\n",
            "      (4): Tanh()\n",
            "      (5): Dropout(p=0.1, inplace=False)\n",
            "      (6): Linear(in_features=50, out_features=10, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (readout_map): FFNN(\n",
            "    (ffnn): Sequential(\n",
            "      (0): Linear(in_features=10, out_features=50, bias=True)\n",
            "      (1): Tanh()\n",
            "      (2): Dropout(p=0.1, inplace=False)\n",
            "      (3): Linear(in_features=50, out_features=50, bias=True)\n",
            "      (4): Tanh()\n",
            "      (5): Dropout(p=0.1, inplace=False)\n",
            "      (6): Linear(in_features=50, out_features=1, bias=True)\n",
            "    )\n",
            "  )\n",
            ") \n",
            "\n",
            "# parameters=10071\n",
            "\n",
            "start training ...\n",
            "0it [00:00, ?it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"demo.py\", line 89, in <module>\n",
            "    plot_only=plot_only)\n",
            "  File \"../NJODE/train.py\", line 512, in train\n",
            "    return_path=False, get_loss=True\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 550, in __call__\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"../NJODE/models.py\", line 439, in forward\n",
            "    last_X=last_X, tau=tau)\n",
            "  File \"../NJODE/models.py\", line 372, in ode_step\n",
            "    h = h + delta_t * self.ode_f(last_X, h, tau, current_time - tau)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 550, in __call__\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"../NJODE/models.py\", line 197, in forward\n",
            "    dim=1)\n",
            "RuntimeError: All input tensors must be on the same device. Received cuda:0 and cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "python demo.py --dataset='Heston'"
      ],
      "metadata": {
        "id": "8-UnPKUeHSXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "eDUsaKe9IH0C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}