{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2022 5 23 STG-NCDE.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMkIwLk2tR5mUYu/2H2dfq0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CristinaMarsh/Reproduce/blob/main/Spatiotemporalgraph/2022_5_23_STG_NCDE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgzGqyZpQnaz",
        "outputId": "1fba8e9d-2b4d-47c8-833c-cbbca33fea6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'STG-NCDE'...\n",
            "remote: Enumerating objects: 43, done.\u001b[K\n",
            "remote: Counting objects: 100% (43/43), done.\u001b[K\n",
            "remote: Compressing objects: 100% (39/39), done.\u001b[K\n",
            "remote: Total 43 (delta 7), reused 36 (delta 3), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (43/43), done.\n"
          ]
        }
      ],
      "source": [
        "! git clone https://github.com/jeongwhanchoi/STG-NCDE.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "- 可以跑，就是慢"
      ],
      "metadata": {
        "id": "o1M5jmzc-kNR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd STG-NCDE/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RcQ2kdp2Qsav",
        "outputId": "365d6a70-21f5-4421-f434-24d99911de3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/STG-NCDE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install torchdiffeq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKk7b_VCQ2Xx",
        "outputId": "3d55e506-2e7e-44a8-c8fa-922ab17604ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchdiffeq\n",
            "  Downloading torchdiffeq-0.2.3-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from torchdiffeq) (1.11.0+cu113)\n",
            "Requirement already satisfied: scipy>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from torchdiffeq) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy>=1.4.0->torchdiffeq) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3.0->torchdiffeq) (4.2.0)\n",
            "Installing collected packages: torchdiffeq\n",
            "Successfully installed torchdiffeq-0.2.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash /content/STG-NCDE/run.sh\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ToeoWSbJQxOv",
        "outputId": "f080520a-9417-443d-d98b-07491b0c7e62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/STG-NCDE\n",
            "Namespace(batch_size=64, cheb_k=2, column_wise=False, comment='', cuda=True, dataset='PEMSD4', debug=False, default_graph=True, device=0, early_stop=True, early_stop_patience=15, embed_dim=10, epochs=200, g_type='agc', grad_norm=False, hid_dim=64, hid_hid_dim=64, horizon=12, input_dim=2, lag=12, log_dir='../runs', log_step=20, loss_func='mae', lr_decay=False, lr_decay_rate=0.3, lr_decay_step='5,20,40,70', lr_init=0.001, mae_thresh=None, mape_thresh=0.0, max_grad_norm=5, missing_rate=0.1, missing_test=False, mode='train', model='GCDE', model_path='', model_type='type1', normalizer='std', num_layers=2, num_nodes=307, output_dim=1, plot=False, real_value=True, seed=10, solver='rk4', teacher_forcing=False, tensorboard=True, test_ratio=0.2, tod=False, val_ratio=0.2, weight_decay=0.001)\n",
            "NeuralGCDE(\n",
            "  (func_f): FinalTanh_f(\n",
            "    input_channels: 2, hidden_channels: 64, hidden_hidden_channels: 64, num_hidden_layers: 2\n",
            "    (linear_in): Linear(in_features=64, out_features=64, bias=True)\n",
            "    (linears): ModuleList(\n",
            "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
            "    )\n",
            "    (linear_out): Linear(in_features=64, out_features=128, bias=True)\n",
            "  )\n",
            "  (func_g): VectorField_g(\n",
            "    input_channels: 2, hidden_channels: 64, hidden_hidden_channels: 64, num_hidden_layers: 2\n",
            "    (linear_in): Linear(in_features=64, out_features=64, bias=True)\n",
            "    (linear_out): Linear(in_features=64, out_features=4096, bias=True)\n",
            "  )\n",
            "  (end_conv): Conv2d(1, 12, kernel_size=(1, 64), stride=(1, 1))\n",
            "  (initial_h): Linear(in_features=2, out_features=64, bias=True)\n",
            "  (initial_z): Linear(in_features=2, out_features=64, bias=True)\n",
            ")\n",
            "*****************Model Parameter*****************\n",
            "node_embeddings torch.Size([307, 10]) True\n",
            "func_f.linear_in.weight torch.Size([64, 64]) True\n",
            "func_f.linear_in.bias torch.Size([64]) True\n",
            "func_f.linears.0.weight torch.Size([64, 64]) True\n",
            "func_f.linears.0.bias torch.Size([64]) True\n",
            "func_f.linear_out.weight torch.Size([128, 64]) True\n",
            "func_f.linear_out.bias torch.Size([128]) True\n",
            "func_g.node_embeddings torch.Size([307, 10]) True\n",
            "func_g.weights_pool torch.Size([10, 2, 64, 64]) True\n",
            "func_g.bias_pool torch.Size([10, 64]) True\n",
            "func_g.linear_in.weight torch.Size([64, 64]) True\n",
            "func_g.linear_in.bias torch.Size([64]) True\n",
            "func_g.linear_out.weight torch.Size([4096, 64]) True\n",
            "func_g.linear_out.bias torch.Size([4096]) True\n",
            "end_conv.weight torch.Size([12, 1, 1, 64]) True\n",
            "end_conv.bias torch.Size([12]) True\n",
            "initial_h.weight torch.Size([64, 2]) True\n",
            "initial_h.bias torch.Size([64]) True\n",
            "initial_z.weight torch.Size([64, 2]) True\n",
            "initial_z.bias torch.Size([64]) True\n",
            "Total params num: 376904\n",
            "*****************Finish Parameter****************\n",
            "Load PEMSD4 Dataset shaped:  (16992, 307, 1) 919.0 0.0 211.7007794815878 180.0\n",
            "Normalize the dataset by Standard Normalization\n",
            "Train:  (10173, 12, 307, 1) (10173, 12, 307, 1)\n",
            "Val:  (3375, 12, 307, 1) (3375, 12, 307, 1)\n",
            "Test:  (3375, 12, 307, 1) (3375, 12, 307, 1)\n",
            "Creat Log File in:  ../runs/PEMSD4/05-23-18h52m_PEMSD4_GCDE_type1_embed{10}hid{64}hidhid{64}lyrs{2}lr{0.001}wd{0.001}/run.log\n",
            "2022-05-23 18:53: Experiment log path in: ../runs/PEMSD4/05-23-18h52m_PEMSD4_GCDE_type1_embed{10}hid{64}hidhid{64}lyrs{2}lr{0.001}wd{0.001}\n",
            "*****************Model Parameter*****************\n",
            "node_embeddings torch.Size([307, 10]) True\n",
            "func_f.linear_in.weight torch.Size([64, 64]) True\n",
            "func_f.linear_in.bias torch.Size([64]) True\n",
            "func_f.linears.0.weight torch.Size([64, 64]) True\n",
            "func_f.linears.0.bias torch.Size([64]) True\n",
            "func_f.linear_out.weight torch.Size([128, 64]) True\n",
            "func_f.linear_out.bias torch.Size([128]) True\n",
            "func_g.node_embeddings torch.Size([307, 10]) True\n",
            "func_g.weights_pool torch.Size([10, 2, 64, 64]) True\n",
            "func_g.bias_pool torch.Size([10, 64]) True\n",
            "func_g.linear_in.weight torch.Size([64, 64]) True\n",
            "func_g.linear_in.bias torch.Size([64]) True\n",
            "func_g.linear_out.weight torch.Size([4096, 64]) True\n",
            "func_g.linear_out.bias torch.Size([4096]) True\n",
            "end_conv.weight torch.Size([12, 1, 1, 64]) True\n",
            "end_conv.bias torch.Size([12]) True\n",
            "initial_h.weight torch.Size([64, 2]) True\n",
            "initial_h.bias torch.Size([64]) True\n",
            "initial_z.weight torch.Size([64, 2]) True\n",
            "initial_z.bias torch.Size([64]) True\n",
            "Total params num: 376904\n",
            "*****************Finish Parameter****************\n",
            "2022-05-23 18:53: Argument batch_size: 64\n",
            "2022-05-23 18:53: Argument cheb_k: 2\n",
            "2022-05-23 18:53: Argument column_wise: False\n",
            "2022-05-23 18:53: Argument comment: ''\n",
            "2022-05-23 18:53: Argument cuda: True\n",
            "2022-05-23 18:53: Argument dataset: 'PEMSD4'\n",
            "2022-05-23 18:53: Argument debug: False\n",
            "2022-05-23 18:53: Argument default_graph: True\n",
            "2022-05-23 18:53: Argument device: 0\n",
            "2022-05-23 18:53: Argument early_stop: True\n",
            "2022-05-23 18:53: Argument early_stop_patience: 15\n",
            "2022-05-23 18:53: Argument embed_dim: 10\n",
            "2022-05-23 18:53: Argument epochs: 200\n",
            "2022-05-23 18:53: Argument g_type: 'agc'\n",
            "2022-05-23 18:53: Argument grad_norm: False\n",
            "2022-05-23 18:53: Argument hid_dim: 64\n",
            "2022-05-23 18:53: Argument hid_hid_dim: 64\n",
            "2022-05-23 18:53: Argument horizon: 12\n",
            "2022-05-23 18:53: Argument input_dim: 2\n",
            "2022-05-23 18:53: Argument lag: 12\n",
            "2022-05-23 18:53: Argument log_dir: '../runs/PEMSD4/05-23-18h52m_PEMSD4_GCDE_type1_embed{10}hid{64}hidhid{64}lyrs{2}lr{0.001}wd{0.001}'\n",
            "2022-05-23 18:53: Argument log_step: 20\n",
            "2022-05-23 18:53: Argument loss_func: 'mae'\n",
            "2022-05-23 18:53: Argument lr_decay: False\n",
            "2022-05-23 18:53: Argument lr_decay_rate: 0.3\n",
            "2022-05-23 18:53: Argument lr_decay_step: '5,20,40,70'\n",
            "2022-05-23 18:53: Argument lr_init: 0.001\n",
            "2022-05-23 18:53: Argument mae_thresh: None\n",
            "2022-05-23 18:53: Argument mape_thresh: 0.0\n",
            "2022-05-23 18:53: Argument max_grad_norm: 5\n",
            "2022-05-23 18:53: Argument missing_rate: 0.1\n",
            "2022-05-23 18:53: Argument missing_test: False\n",
            "2022-05-23 18:53: Argument mode: 'train'\n",
            "2022-05-23 18:53: Argument model: 'GCDE'\n",
            "2022-05-23 18:53: Argument model_path: ''\n",
            "2022-05-23 18:53: Argument model_type: 'type1'\n",
            "2022-05-23 18:53: Argument normalizer: 'std'\n",
            "2022-05-23 18:53: Argument num_layers: 2\n",
            "2022-05-23 18:53: Argument num_nodes: 307\n",
            "2022-05-23 18:53: Argument output_dim: 1\n",
            "2022-05-23 18:53: Argument plot: False\n",
            "2022-05-23 18:53: Argument real_value: True\n",
            "2022-05-23 18:53: Argument seed: 10\n",
            "2022-05-23 18:53: Argument solver: 'rk4'\n",
            "2022-05-23 18:53: Argument teacher_forcing: False\n",
            "2022-05-23 18:53: Argument tensorboard: True\n",
            "2022-05-23 18:53: Argument test_ratio: 0.2\n",
            "2022-05-23 18:53: Argument tod: False\n",
            "2022-05-23 18:53: Argument val_ratio: 0.2\n",
            "2022-05-23 18:53: Argument weight_decay: 0.001\n",
            "2022-05-23 18:53: NeuralGCDE(\n",
            "  (func_f): FinalTanh_f(\n",
            "    input_channels: 2, hidden_channels: 64, hidden_hidden_channels: 64, num_hidden_layers: 2\n",
            "    (linear_in): Linear(in_features=64, out_features=64, bias=True)\n",
            "    (linears): ModuleList(\n",
            "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
            "    )\n",
            "    (linear_out): Linear(in_features=64, out_features=128, bias=True)\n",
            "  )\n",
            "  (func_g): VectorField_g(\n",
            "    input_channels: 2, hidden_channels: 64, hidden_hidden_channels: 64, num_hidden_layers: 2\n",
            "    (linear_in): Linear(in_features=64, out_features=64, bias=True)\n",
            "    (linear_out): Linear(in_features=64, out_features=4096, bias=True)\n",
            "  )\n",
            "  (end_conv): Conv2d(1, 12, kernel_size=(1, 64), stride=(1, 1))\n",
            "  (initial_h): Linear(in_features=2, out_features=64, bias=True)\n",
            "  (initial_z): Linear(in_features=2, out_features=64, bias=True)\n",
            ")\n",
            "2022-05-23 18:53: Total params: 376904\n",
            "2022-05-23 18:53: Train Epoch 1: 0/158 Loss: 219.690979\n",
            "2022-05-23 18:53: Train Epoch 1: 20/158 Loss: 69.501938\n",
            "2022-05-23 18:54: Train Epoch 1: 40/158 Loss: 38.752708\n",
            "2022-05-23 18:54: Train Epoch 1: 60/158 Loss: 29.850138\n",
            "2022-05-23 18:55: Train Epoch 1: 80/158 Loss: 28.658676\n",
            "2022-05-23 18:55: Train Epoch 1: 100/158 Loss: 31.003225\n",
            "2022-05-23 18:56: Train Epoch 1: 120/158 Loss: 29.156040\n",
            "2022-05-23 18:56: Train Epoch 1: 140/158 Loss: 29.959833\n",
            "2022-05-23 18:57: **********Train Epoch 1: averaged Loss: 47.512464\n",
            "2022-05-23 18:57: **********Val Epoch 1: average Loss: 29.777617\n",
            "2022-05-23 18:57: *********************************Current best model saved!\n",
            "2022-05-23 18:57: Train Epoch 2: 0/158 Loss: 30.375723\n",
            "2022-05-23 18:58: Train Epoch 2: 20/158 Loss: 29.311234\n",
            "2022-05-23 18:58: Train Epoch 2: 40/158 Loss: 28.443932\n",
            "2022-05-23 18:59: Train Epoch 2: 60/158 Loss: 29.617901\n",
            "2022-05-23 18:59: Train Epoch 2: 80/158 Loss: 25.420603\n",
            "2022-05-23 19:00: Train Epoch 2: 100/158 Loss: 31.136648\n",
            "2022-05-23 19:00: Train Epoch 2: 120/158 Loss: 25.989733\n",
            "2022-05-23 19:01: Train Epoch 2: 140/158 Loss: 24.757690\n",
            "2022-05-23 19:01: **********Train Epoch 2: averaged Loss: 27.684693\n",
            "2022-05-23 19:02: **********Val Epoch 2: average Loss: 26.851823\n",
            "2022-05-23 19:02: *********************************Current best model saved!\n",
            "2022-05-23 19:02: Train Epoch 3: 0/158 Loss: 25.962198\n",
            "2022-05-23 19:02: Train Epoch 3: 20/158 Loss: 33.473831\n",
            "2022-05-23 19:03: Train Epoch 3: 40/158 Loss: 23.192219\n",
            "2022-05-23 19:03: Train Epoch 3: 60/158 Loss: 24.879791\n",
            "2022-05-23 19:04: Train Epoch 3: 80/158 Loss: 22.825916\n",
            "2022-05-23 19:04: Train Epoch 3: 100/158 Loss: 24.962305\n",
            "2022-05-23 19:05: Train Epoch 3: 120/158 Loss: 25.264614\n",
            "2022-05-23 19:06: Train Epoch 3: 140/158 Loss: 25.481024\n",
            "2022-05-23 19:06: **********Train Epoch 3: averaged Loss: 26.011769\n",
            "2022-05-23 19:06: **********Val Epoch 3: average Loss: 26.012800\n",
            "2022-05-23 19:06: *********************************Current best model saved!\n",
            "2022-05-23 19:06: Train Epoch 4: 0/158 Loss: 25.621185\n",
            "2022-05-23 19:07: Train Epoch 4: 20/158 Loss: 25.518450\n",
            "2022-05-23 19:07: Train Epoch 4: 40/158 Loss: 26.718908\n",
            "2022-05-23 19:08: Train Epoch 4: 60/158 Loss: 26.454077\n",
            "2022-05-23 19:09: Train Epoch 4: 80/158 Loss: 24.142658\n",
            "2022-05-23 19:09: Train Epoch 4: 100/158 Loss: 23.980032\n",
            "2022-05-23 19:10: Train Epoch 4: 120/158 Loss: 25.516491\n",
            "2022-05-23 19:10: Train Epoch 4: 140/158 Loss: 25.606005\n",
            "2022-05-23 19:11: **********Train Epoch 4: averaged Loss: 24.623064\n",
            "2022-05-23 19:11: **********Val Epoch 4: average Loss: 25.598546\n",
            "2022-05-23 19:11: *********************************Current best model saved!\n",
            "2022-05-23 19:11: Train Epoch 5: 0/158 Loss: 22.677063\n",
            "2022-05-23 19:12: Train Epoch 5: 20/158 Loss: 24.815283\n",
            "2022-05-23 19:12: Train Epoch 5: 40/158 Loss: 26.773052\n",
            "2022-05-23 19:13: Train Epoch 5: 60/158 Loss: 22.797173\n",
            "2022-05-23 19:13: Train Epoch 5: 80/158 Loss: 25.570412\n",
            "2022-05-23 19:14: Train Epoch 5: 100/158 Loss: 25.116604\n",
            "2022-05-23 19:14: Train Epoch 5: 120/158 Loss: 21.537447\n",
            "2022-05-23 19:15: Train Epoch 5: 140/158 Loss: 21.998026\n",
            "2022-05-23 19:15: **********Train Epoch 5: averaged Loss: 24.008995\n",
            "2022-05-23 19:16: **********Val Epoch 5: average Loss: 24.239724\n",
            "2022-05-23 19:16: *********************************Current best model saved!\n",
            "2022-05-23 19:16: Train Epoch 6: 0/158 Loss: 22.461960\n",
            "2022-05-23 19:16: Train Epoch 6: 20/158 Loss: 22.944862\n",
            "2022-05-23 19:17: Train Epoch 6: 40/158 Loss: 24.279263\n",
            "2022-05-23 19:17: Train Epoch 6: 60/158 Loss: 22.589035\n",
            "2022-05-23 19:18: Train Epoch 6: 80/158 Loss: 23.384731\n",
            "2022-05-23 19:18: Train Epoch 6: 100/158 Loss: 24.032469\n",
            "2022-05-23 19:19: Train Epoch 6: 120/158 Loss: 22.477453\n",
            "2022-05-23 19:19: Train Epoch 6: 140/158 Loss: 22.364222\n",
            "2022-05-23 19:20: **********Train Epoch 6: averaged Loss: 23.151302\n",
            "2022-05-23 19:20: **********Val Epoch 6: average Loss: 23.678205\n",
            "2022-05-23 19:20: *********************************Current best model saved!\n",
            "2022-05-23 19:20: Train Epoch 7: 0/158 Loss: 22.753843\n",
            "2022-05-23 19:21: Train Epoch 7: 20/158 Loss: 22.035862\n",
            "2022-05-23 19:21: Train Epoch 7: 40/158 Loss: 22.270584\n",
            "2022-05-23 19:22: Train Epoch 7: 60/158 Loss: 23.780012\n",
            "2022-05-23 19:22: Train Epoch 7: 80/158 Loss: 22.774441\n",
            "2022-05-23 19:23: Train Epoch 7: 100/158 Loss: 22.734856\n",
            "2022-05-23 19:23: Train Epoch 7: 120/158 Loss: 24.164953\n",
            "2022-05-23 19:24: Train Epoch 7: 140/158 Loss: 23.853405\n",
            "2022-05-23 19:24: **********Train Epoch 7: averaged Loss: 22.777827\n",
            "2022-05-23 19:25: **********Val Epoch 7: average Loss: 23.366964\n",
            "2022-05-23 19:25: *********************************Current best model saved!\n",
            "2022-05-23 19:25: Train Epoch 8: 0/158 Loss: 23.163940\n",
            "2022-05-23 19:25: Train Epoch 8: 20/158 Loss: 22.019484\n",
            "2022-05-23 19:26: Train Epoch 8: 40/158 Loss: 22.637194\n",
            "2022-05-23 19:26: Train Epoch 8: 60/158 Loss: 25.123894\n",
            "2022-05-23 19:27: Train Epoch 8: 80/158 Loss: 21.754673\n",
            "2022-05-23 19:27: Train Epoch 8: 100/158 Loss: 22.220652\n",
            "2022-05-23 19:28: Train Epoch 8: 120/158 Loss: 22.026934\n",
            "2022-05-23 19:29: Train Epoch 8: 140/158 Loss: 22.375143\n",
            "2022-05-23 19:29: **********Train Epoch 8: averaged Loss: 22.849513\n",
            "2022-05-23 19:29: **********Val Epoch 8: average Loss: 23.792607\n",
            "2022-05-23 19:29: Train Epoch 9: 0/158 Loss: 21.524147\n",
            "2022-05-23 19:30: Train Epoch 9: 20/158 Loss: 21.112814\n",
            "2022-05-23 19:30: Train Epoch 9: 40/158 Loss: 22.899113\n",
            "2022-05-23 19:31: Train Epoch 9: 60/158 Loss: 24.736174\n",
            "2022-05-23 19:32: Train Epoch 9: 80/158 Loss: 22.860266\n",
            "2022-05-23 19:32: Train Epoch 9: 100/158 Loss: 22.623253\n",
            "2022-05-23 19:33: Train Epoch 9: 120/158 Loss: 22.093111\n",
            "2022-05-23 19:33: Train Epoch 9: 140/158 Loss: 24.125761\n",
            "2022-05-23 19:34: **********Train Epoch 9: averaged Loss: 22.152514\n",
            "2022-05-23 19:34: **********Val Epoch 9: average Loss: 22.923440\n",
            "2022-05-23 19:34: *********************************Current best model saved!\n",
            "2022-05-23 19:34: Train Epoch 10: 0/158 Loss: 22.251926\n",
            "2022-05-23 19:35: Train Epoch 10: 20/158 Loss: 18.669617\n",
            "2022-05-23 19:35: Train Epoch 10: 40/158 Loss: 21.473848\n",
            "2022-05-23 19:36: Train Epoch 10: 60/158 Loss: 23.060436\n",
            "2022-05-23 19:36: Train Epoch 10: 80/158 Loss: 22.892134\n",
            "2022-05-23 19:37: Train Epoch 10: 100/158 Loss: 24.945971\n",
            "2022-05-23 19:37: Train Epoch 10: 120/158 Loss: 22.074644\n",
            "2022-05-23 19:38: Train Epoch 10: 140/158 Loss: 20.245285\n",
            "2022-05-23 19:38: **********Train Epoch 10: averaged Loss: 21.906426\n",
            "2022-05-23 19:39: **********Val Epoch 10: average Loss: 22.467437\n",
            "2022-05-23 19:39: *********************************Current best model saved!\n",
            "2022-05-23 19:39: Train Epoch 11: 0/158 Loss: 20.176311\n",
            "2022-05-23 19:39: Train Epoch 11: 20/158 Loss: 22.876341\n",
            "2022-05-23 19:40: Train Epoch 11: 40/158 Loss: 21.454016\n",
            "2022-05-23 19:40: Train Epoch 11: 60/158 Loss: 20.713648\n",
            "2022-05-23 19:41: Train Epoch 11: 80/158 Loss: 22.873585\n",
            "2022-05-23 19:41: Train Epoch 11: 100/158 Loss: 22.836695\n",
            "2022-05-23 19:42: Train Epoch 11: 120/158 Loss: 21.111057\n",
            "2022-05-23 19:42: Train Epoch 11: 140/158 Loss: 21.606861\n",
            "2022-05-23 19:43: **********Train Epoch 11: averaged Loss: 21.698587\n",
            "2022-05-23 19:43: **********Val Epoch 11: average Loss: 22.039861\n",
            "2022-05-23 19:43: *********************************Current best model saved!\n",
            "2022-05-23 19:43: Train Epoch 12: 0/158 Loss: 22.064697\n",
            "2022-05-23 19:44: Train Epoch 12: 20/158 Loss: 22.829729\n",
            "2022-05-23 19:44: Train Epoch 12: 40/158 Loss: 24.116594\n",
            "2022-05-23 19:45: Train Epoch 12: 60/158 Loss: 20.927593\n",
            "2022-05-23 19:45: Train Epoch 12: 80/158 Loss: 20.529007\n",
            "2022-05-23 19:46: Train Epoch 12: 100/158 Loss: 22.591423\n",
            "2022-05-23 19:46: Train Epoch 12: 120/158 Loss: 21.562984\n",
            "2022-05-23 19:47: Train Epoch 12: 140/158 Loss: 20.441811\n",
            "2022-05-23 19:47: **********Train Epoch 12: averaged Loss: 21.599753\n",
            "2022-05-23 19:48: **********Val Epoch 12: average Loss: 21.932109\n",
            "2022-05-23 19:48: *********************************Current best model saved!\n",
            "2022-05-23 19:48: Train Epoch 13: 0/158 Loss: 21.529715\n",
            "2022-05-23 19:48: Train Epoch 13: 20/158 Loss: 21.924582\n",
            "2022-05-23 19:49: Train Epoch 13: 40/158 Loss: 21.035980\n",
            "2022-05-23 19:49: Train Epoch 13: 60/158 Loss: 21.754740\n",
            "2022-05-23 19:50: Train Epoch 13: 80/158 Loss: 19.615585\n",
            "2022-05-23 19:50: Train Epoch 13: 100/158 Loss: 20.345676\n",
            "2022-05-23 19:51: Train Epoch 13: 120/158 Loss: 22.845142\n",
            "2022-05-23 19:52: Train Epoch 13: 140/158 Loss: 19.002289\n",
            "2022-05-23 19:52: **********Train Epoch 13: averaged Loss: 21.032333\n",
            "2022-05-23 19:52: **********Val Epoch 13: average Loss: 24.234906\n",
            "2022-05-23 19:52: Train Epoch 14: 0/158 Loss: 24.628397\n",
            "2022-05-23 19:53: Train Epoch 14: 20/158 Loss: 21.147030\n",
            "2022-05-23 19:53: Train Epoch 14: 40/158 Loss: 22.673000\n",
            "2022-05-23 19:54: Train Epoch 14: 60/158 Loss: 20.621265\n",
            "2022-05-23 19:55: Train Epoch 14: 80/158 Loss: 21.157524\n",
            "2022-05-23 19:55: Train Epoch 14: 100/158 Loss: 18.505455\n",
            "2022-05-23 19:56: Train Epoch 14: 120/158 Loss: 19.460440\n",
            "2022-05-23 19:56: Train Epoch 14: 140/158 Loss: 20.609919\n",
            "2022-05-23 19:57: **********Train Epoch 14: averaged Loss: 21.293805\n",
            "2022-05-23 19:57: **********Val Epoch 14: average Loss: 21.849686\n",
            "2022-05-23 19:57: *********************************Current best model saved!\n",
            "2022-05-23 19:57: Train Epoch 15: 0/158 Loss: 20.085365\n",
            "2022-05-23 19:58: Train Epoch 15: 20/158 Loss: 21.205030\n",
            "2022-05-23 19:58: Train Epoch 15: 40/158 Loss: 21.758162\n",
            "2022-05-23 19:59: Train Epoch 15: 60/158 Loss: 21.170895\n",
            "2022-05-23 19:59: Train Epoch 15: 80/158 Loss: 18.754114\n",
            "2022-05-23 20:00: Train Epoch 15: 100/158 Loss: 20.354294\n",
            "2022-05-23 20:00: Train Epoch 15: 120/158 Loss: 21.996761\n",
            "2022-05-23 20:01: Train Epoch 15: 140/158 Loss: 19.631706\n",
            "2022-05-23 20:01: **********Train Epoch 15: averaged Loss: 20.729086\n",
            "2022-05-23 20:02: **********Val Epoch 15: average Loss: 21.259343\n",
            "2022-05-23 20:02: *********************************Current best model saved!\n",
            "2022-05-23 20:02: Train Epoch 16: 0/158 Loss: 20.385231\n",
            "2022-05-23 20:02: Train Epoch 16: 20/158 Loss: 19.415819\n",
            "2022-05-23 20:03: Train Epoch 16: 40/158 Loss: 21.490599\n",
            "2022-05-23 20:03: Train Epoch 16: 60/158 Loss: 20.451845\n",
            "2022-05-23 20:04: Train Epoch 16: 80/158 Loss: 19.664484\n",
            "2022-05-23 20:04: Train Epoch 16: 100/158 Loss: 20.052382\n",
            "2022-05-23 20:05: Train Epoch 16: 120/158 Loss: 20.102325\n",
            "2022-05-23 20:05: Train Epoch 16: 140/158 Loss: 22.090641\n",
            "2022-05-23 20:06: **********Train Epoch 16: averaged Loss: 20.883826\n",
            "2022-05-23 20:06: **********Val Epoch 16: average Loss: 21.170223\n",
            "2022-05-23 20:06: *********************************Current best model saved!\n",
            "2022-05-23 20:06: Train Epoch 17: 0/158 Loss: 20.763121\n",
            "2022-05-23 20:07: Train Epoch 17: 20/158 Loss: 20.849226\n",
            "2022-05-23 20:07: Train Epoch 17: 40/158 Loss: 20.106543\n",
            "2022-05-23 20:08: Train Epoch 17: 60/158 Loss: 23.934956\n",
            "2022-05-23 20:08: Train Epoch 17: 80/158 Loss: 20.565809\n",
            "2022-05-23 20:09: Train Epoch 17: 100/158 Loss: 19.856915\n",
            "2022-05-23 20:09: Train Epoch 17: 120/158 Loss: 21.199776\n",
            "2022-05-23 20:10: Train Epoch 17: 140/158 Loss: 18.610479\n",
            "2022-05-23 20:10: **********Train Epoch 17: averaged Loss: 20.444007\n",
            "2022-05-23 20:11: **********Val Epoch 17: average Loss: 20.995849\n",
            "2022-05-23 20:11: *********************************Current best model saved!\n",
            "2022-05-23 20:11: Train Epoch 18: 0/158 Loss: 20.084440\n",
            "2022-05-23 20:11: Train Epoch 18: 20/158 Loss: 18.411596\n",
            "2022-05-23 20:12: Train Epoch 18: 40/158 Loss: 19.658697\n",
            "2022-05-23 20:12: Train Epoch 18: 60/158 Loss: 20.030079\n",
            "2022-05-23 20:13: Train Epoch 18: 80/158 Loss: 19.499119\n",
            "2022-05-23 20:13: Train Epoch 18: 100/158 Loss: 21.982718\n",
            "2022-05-23 20:14: Train Epoch 18: 120/158 Loss: 18.625566\n",
            "2022-05-23 20:15: Train Epoch 18: 140/158 Loss: 20.858786\n",
            "2022-05-23 20:15: **********Train Epoch 18: averaged Loss: 20.356380\n",
            "2022-05-23 20:15: **********Val Epoch 18: average Loss: 21.370117\n",
            "2022-05-23 20:15: Train Epoch 19: 0/158 Loss: 21.002499\n",
            "2022-05-23 20:16: Train Epoch 19: 20/158 Loss: 22.744766\n",
            "2022-05-23 20:16: Train Epoch 19: 40/158 Loss: 21.301502\n",
            "2022-05-23 20:17: Train Epoch 19: 60/158 Loss: 20.054516\n",
            "2022-05-23 20:18: Train Epoch 19: 80/158 Loss: 19.790489\n",
            "2022-05-23 20:18: Train Epoch 19: 100/158 Loss: 21.506491\n",
            "2022-05-23 20:19: Train Epoch 19: 120/158 Loss: 21.644278\n",
            "2022-05-23 20:19: Train Epoch 19: 140/158 Loss: 19.442654\n",
            "2022-05-23 20:20: **********Train Epoch 19: averaged Loss: 20.246108\n",
            "2022-05-23 20:20: **********Val Epoch 19: average Loss: 21.202741\n",
            "2022-05-23 20:20: Train Epoch 20: 0/158 Loss: 19.879753\n",
            "2022-05-23 20:21: Train Epoch 20: 20/158 Loss: 19.871841\n",
            "2022-05-23 20:21: Train Epoch 20: 40/158 Loss: 20.149057\n",
            "2022-05-23 20:22: Train Epoch 20: 60/158 Loss: 19.055588\n",
            "2022-05-23 20:22: Train Epoch 20: 80/158 Loss: 20.271503\n",
            "2022-05-23 20:23: Train Epoch 20: 100/158 Loss: 21.941393\n",
            "2022-05-23 20:23: Train Epoch 20: 120/158 Loss: 19.354368\n",
            "2022-05-23 20:24: Train Epoch 20: 140/158 Loss: 20.897840\n",
            "2022-05-23 20:24: **********Train Epoch 20: averaged Loss: 19.979932\n",
            "2022-05-23 20:25: **********Val Epoch 20: average Loss: 21.369544\n",
            "2022-05-23 20:25: Train Epoch 21: 0/158 Loss: 19.828106\n",
            "2022-05-23 20:25: Train Epoch 21: 20/158 Loss: 19.631058\n",
            "2022-05-23 20:26: Train Epoch 21: 40/158 Loss: 19.471373\n",
            "2022-05-23 20:26: Train Epoch 21: 60/158 Loss: 19.192493\n",
            "2022-05-23 20:27: Train Epoch 21: 80/158 Loss: 18.315346\n",
            "2022-05-23 20:27: Train Epoch 21: 100/158 Loss: 21.267881\n",
            "2022-05-23 20:28: Train Epoch 21: 120/158 Loss: 19.975681\n",
            "2022-05-23 20:28: Train Epoch 21: 140/158 Loss: 21.030535\n",
            "2022-05-23 20:29: **********Train Epoch 21: averaged Loss: 19.807159\n",
            "2022-05-23 20:29: **********Val Epoch 21: average Loss: 20.616615\n",
            "2022-05-23 20:29: *********************************Current best model saved!\n",
            "2022-05-23 20:29: Train Epoch 22: 0/158 Loss: 19.192959\n",
            "2022-05-23 20:30: Train Epoch 22: 20/158 Loss: 19.578562\n",
            "2022-05-23 20:30: Train Epoch 22: 40/158 Loss: 19.261292\n",
            "2022-05-23 20:31: Train Epoch 22: 60/158 Loss: 19.758648\n",
            "2022-05-23 20:31: Train Epoch 22: 80/158 Loss: 20.272982\n",
            "2022-05-23 20:32: Train Epoch 22: 100/158 Loss: 20.657993\n",
            "2022-05-23 20:32: Train Epoch 22: 120/158 Loss: 19.773132\n",
            "2022-05-23 20:33: Train Epoch 22: 140/158 Loss: 19.527924\n",
            "2022-05-23 20:33: **********Train Epoch 22: averaged Loss: 19.647796\n",
            "2022-05-23 20:34: **********Val Epoch 22: average Loss: 20.728517\n",
            "2022-05-23 20:34: Train Epoch 23: 0/158 Loss: 19.971834\n",
            "2022-05-23 20:34: Train Epoch 23: 20/158 Loss: 21.462473\n",
            "2022-05-23 20:35: Train Epoch 23: 40/158 Loss: 19.321924\n",
            "2022-05-23 20:35: Train Epoch 23: 60/158 Loss: 19.536489\n",
            "2022-05-23 20:36: Train Epoch 23: 80/158 Loss: 18.509136\n",
            "2022-05-23 20:36: Train Epoch 23: 100/158 Loss: 20.269676\n",
            "2022-05-23 20:37: Train Epoch 23: 120/158 Loss: 20.215618\n",
            "2022-05-23 20:38: Train Epoch 23: 140/158 Loss: 18.697725\n",
            "2022-05-23 20:38: **********Train Epoch 23: averaged Loss: 19.642471\n",
            "2022-05-23 20:38: **********Val Epoch 23: average Loss: 21.450941\n",
            "2022-05-23 20:38: Train Epoch 24: 0/158 Loss: 17.916035\n",
            "2022-05-23 20:39: Train Epoch 24: 20/158 Loss: 19.803322\n",
            "2022-05-23 20:39: Train Epoch 24: 40/158 Loss: 18.691912\n",
            "2022-05-23 20:40: Train Epoch 24: 60/158 Loss: 19.765469\n",
            "2022-05-23 20:41: Train Epoch 24: 80/158 Loss: 20.534719\n",
            "2022-05-23 20:41: Train Epoch 24: 100/158 Loss: 20.707827\n",
            "2022-05-23 20:42: Train Epoch 24: 120/158 Loss: 20.379789\n",
            "2022-05-23 20:42: Train Epoch 24: 140/158 Loss: 21.703152\n",
            "2022-05-23 20:43: **********Train Epoch 24: averaged Loss: 19.634565\n",
            "2022-05-23 20:43: **********Val Epoch 24: average Loss: 20.527511\n",
            "2022-05-23 20:43: *********************************Current best model saved!\n",
            "2022-05-23 20:43: Train Epoch 25: 0/158 Loss: 18.695707\n",
            "2022-05-23 20:44: Train Epoch 25: 20/158 Loss: 18.064278\n",
            "2022-05-23 20:44: Train Epoch 25: 40/158 Loss: 20.694363\n",
            "2022-05-23 20:45: Train Epoch 25: 60/158 Loss: 20.950945\n",
            "2022-05-23 20:45: Train Epoch 25: 80/158 Loss: 19.817862\n",
            "2022-05-23 20:46: Train Epoch 25: 100/158 Loss: 19.004919\n",
            "2022-05-23 20:46: Train Epoch 25: 120/158 Loss: 20.825098\n",
            "2022-05-23 20:47: Train Epoch 25: 140/158 Loss: 19.830065\n",
            "2022-05-23 20:47: **********Train Epoch 25: averaged Loss: 19.381307\n",
            "2022-05-23 20:48: **********Val Epoch 25: average Loss: 20.571525\n",
            "2022-05-23 20:48: Train Epoch 26: 0/158 Loss: 18.764133\n",
            "2022-05-23 20:48: Train Epoch 26: 20/158 Loss: 19.653961\n",
            "2022-05-23 20:49: Train Epoch 26: 40/158 Loss: 18.744623\n",
            "2022-05-23 20:49: Train Epoch 26: 60/158 Loss: 18.349102\n",
            "2022-05-23 20:50: Train Epoch 26: 80/158 Loss: 18.526411\n",
            "2022-05-23 20:50: Train Epoch 26: 100/158 Loss: 19.055693\n",
            "2022-05-23 20:51: Train Epoch 26: 120/158 Loss: 21.482925\n",
            "2022-05-23 20:51: Train Epoch 26: 140/158 Loss: 19.991606\n",
            "2022-05-23 20:52: **********Train Epoch 26: averaged Loss: 19.451829\n",
            "2022-05-23 20:52: **********Val Epoch 26: average Loss: 20.499767\n",
            "2022-05-23 20:52: *********************************Current best model saved!\n",
            "2022-05-23 20:52: Train Epoch 27: 0/158 Loss: 19.330559\n",
            "2022-05-23 20:53: Train Epoch 27: 20/158 Loss: 21.029152\n",
            "2022-05-23 20:53: Train Epoch 27: 40/158 Loss: 19.535822\n",
            "2022-05-23 20:54: Train Epoch 27: 60/158 Loss: 18.968884\n",
            "2022-05-23 20:54: Train Epoch 27: 80/158 Loss: 19.504299\n",
            "2022-05-23 20:55: Train Epoch 27: 100/158 Loss: 19.509075\n",
            "2022-05-23 20:55: Train Epoch 27: 120/158 Loss: 19.678488\n",
            "2022-05-23 20:56: Train Epoch 27: 140/158 Loss: 19.941988\n",
            "2022-05-23 20:56: **********Train Epoch 27: averaged Loss: 19.250094\n",
            "2022-05-23 20:57: **********Val Epoch 27: average Loss: 20.222178\n",
            "2022-05-23 20:57: *********************************Current best model saved!\n",
            "2022-05-23 20:57: Train Epoch 28: 0/158 Loss: 17.881327\n",
            "2022-05-23 20:57: Train Epoch 28: 20/158 Loss: 17.789757\n",
            "2022-05-23 20:58: Train Epoch 28: 40/158 Loss: 20.164034\n",
            "2022-05-23 20:58: Train Epoch 28: 60/158 Loss: 21.217068\n",
            "2022-05-23 20:59: Train Epoch 28: 80/158 Loss: 19.685894\n",
            "2022-05-23 21:00: Train Epoch 28: 100/158 Loss: 19.832468\n",
            "2022-05-23 21:00: Train Epoch 28: 120/158 Loss: 19.860170\n",
            "2022-05-23 21:01: Train Epoch 28: 140/158 Loss: 20.680161\n",
            "2022-05-23 21:01: **********Train Epoch 28: averaged Loss: 19.289541\n",
            "2022-05-23 21:01: **********Val Epoch 28: average Loss: 20.272927\n",
            "2022-05-23 21:02: Train Epoch 29: 0/158 Loss: 20.305706\n",
            "2022-05-23 21:02: Train Epoch 29: 20/158 Loss: 18.926270\n",
            "2022-05-23 21:03: Train Epoch 29: 40/158 Loss: 18.881790\n",
            "2022-05-23 21:03: Train Epoch 29: 60/158 Loss: 19.716795\n",
            "2022-05-23 21:04: Train Epoch 29: 80/158 Loss: 20.622833\n",
            "2022-05-23 21:04: Train Epoch 29: 100/158 Loss: 18.149240\n",
            "2022-05-23 21:05: Train Epoch 29: 120/158 Loss: 19.275152\n",
            "2022-05-23 21:05: Train Epoch 29: 140/158 Loss: 20.194248\n",
            "2022-05-23 21:06: **********Train Epoch 29: averaged Loss: 19.308195\n",
            "2022-05-23 21:06: **********Val Epoch 29: average Loss: 21.746332\n",
            "2022-05-23 21:06: Train Epoch 30: 0/158 Loss: 20.380362\n",
            "2022-05-23 21:07: Train Epoch 30: 20/158 Loss: 18.581549\n",
            "2022-05-23 21:07: Train Epoch 30: 40/158 Loss: 18.291685\n",
            "2022-05-23 21:08: Train Epoch 30: 60/158 Loss: 20.190962\n",
            "2022-05-23 21:08: Train Epoch 30: 80/158 Loss: 19.513288\n",
            "2022-05-23 21:09: Train Epoch 30: 100/158 Loss: 18.986109\n",
            "2022-05-23 21:09: Train Epoch 30: 120/158 Loss: 17.273766\n",
            "2022-05-23 21:10: Train Epoch 30: 140/158 Loss: 20.043686\n",
            "2022-05-23 21:10: **********Train Epoch 30: averaged Loss: 19.263020\n",
            "2022-05-23 21:11: **********Val Epoch 30: average Loss: 19.940010\n",
            "2022-05-23 21:11: *********************************Current best model saved!\n",
            "2022-05-23 21:11: Train Epoch 31: 0/158 Loss: 18.683187\n",
            "2022-05-23 21:11: Train Epoch 31: 20/158 Loss: 20.470306\n",
            "2022-05-23 21:12: Train Epoch 31: 40/158 Loss: 19.719288\n",
            "2022-05-23 21:12: Train Epoch 31: 60/158 Loss: 20.462698\n",
            "2022-05-23 21:13: Train Epoch 31: 80/158 Loss: 16.764099\n",
            "2022-05-23 21:14: Train Epoch 31: 100/158 Loss: 19.960129\n",
            "2022-05-23 21:14: Train Epoch 31: 120/158 Loss: 18.961298\n",
            "2022-05-23 21:15: Train Epoch 31: 140/158 Loss: 19.431540\n",
            "2022-05-23 21:15: **********Train Epoch 31: averaged Loss: 18.948454\n",
            "2022-05-23 21:15: **********Val Epoch 31: average Loss: 19.996969\n",
            "2022-05-23 21:16: Train Epoch 32: 0/158 Loss: 18.901093\n",
            "2022-05-23 21:16: Train Epoch 32: 20/158 Loss: 19.233065\n",
            "2022-05-23 21:17: Train Epoch 32: 40/158 Loss: 20.208973\n",
            "2022-05-23 21:17: Train Epoch 32: 60/158 Loss: 19.702971\n",
            "2022-05-23 21:18: Train Epoch 32: 80/158 Loss: 19.603548\n",
            "2022-05-23 21:18: Train Epoch 32: 100/158 Loss: 20.288839\n",
            "2022-05-23 21:19: Train Epoch 32: 120/158 Loss: 17.890236\n",
            "2022-05-23 21:19: Train Epoch 32: 140/158 Loss: 19.286995\n",
            "2022-05-23 21:20: **********Train Epoch 32: averaged Loss: 19.023087\n",
            "2022-05-23 21:20: **********Val Epoch 32: average Loss: 20.383368\n",
            "2022-05-23 21:20: Train Epoch 33: 0/158 Loss: 17.918758\n",
            "2022-05-23 21:21: Train Epoch 33: 20/158 Loss: 19.671926\n",
            "2022-05-23 21:21: Train Epoch 33: 40/158 Loss: 17.382452\n",
            "2022-05-23 21:22: Train Epoch 33: 60/158 Loss: 18.323137\n",
            "2022-05-23 21:22: Train Epoch 33: 80/158 Loss: 18.344608\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "S-WueMiEQ0U3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}